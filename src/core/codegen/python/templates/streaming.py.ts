/**
 * @file streaming.py.ts
 * @description Python streaming utilities template for Server-Sent Events (SSE)
 * 
 * @architecture Phase 2, Task 2.1 - Streaming Infrastructure
 * @created 2025-12-20
 * @author AI (Cline) + Human Review
 * @confidence 9/10 - SSE spec compliant, tested patterns
 * 
 * @see docs/Catalyst documentation/CATALYST_PHASE_2_TASKS.md - Task 2.1
 * @see docs/Catalyst documentation/CATALYST_SPECIFICATION.md - Section 4.2
 * @see .implementation/Catalyst_tasks/phase-2-llm-integration/task-2.1-streaming-infrastructure.md
 * 
 * PROBLEM SOLVED:
 * - Generate Python code for SSE streaming responses
 * - Support LLM token streaming with minimal latency
 * - Work with nginx reverse proxies (X-Accel-Buffering)
 * - Handle parallel operations with partial results
 * 
 * SOLUTION:
 * - TypeScript templates that output Python functions
 * - Uses orjson for fast JSON serialization (2-3x faster)
 * - Async generators for memory-efficient streaming
 * - Proper SSE event format: event: {type}\ndata: {json}\n\n
 * - Headers prevent buffering: Cache-Control, X-Accel-Buffering
 * 
 * USAGE:
 * This file is imported by StreamingGenerator to generate streaming.py
 * The generated Python code is used by all LLM nodes (Anthropic, OpenAI, Groq)
 * 
 * DESIGN DECISIONS:
 * - orjson over json: 2-3x faster, critical for high-frequency tokens
 * - SSE over WebSockets: Simpler, unidirectional, proxy-friendly
 * - Async generators: Pythonic, backpressure handling, memory efficient
 * - Multiple event types: token, partial, done, error for clarity
 * 
 * @security-critical true - User data flows through streaming
 * @performance-critical true - On critical path for LLM responses
 */

/**
 * Generate complete streaming.py module
 * 
 * This is the main export that combines all streaming utility functions
 * into a complete Python module ready for use in generated workflows.
 * 
 * GENERATED MODULE INCLUDES:
 * - format_sse(): Format data as SSE event
 * - streaming_response(): Wrap generator with proper headers
 * - stream_tokens(): Stream LLM tokens one by one
 * - stream_partial_results(): Stream parallel operation results
 * 
 * @returns Complete Python module as string
 */
export function generateStreamingModule(): string {
  return `"""
Streaming utilities for Server-Sent Events (SSE)

This module provides utilities for streaming responses from FastAPI endpoints.
Critical for LLM token streaming and real-time partial results.

@catalyst:generated
DO NOT EDIT - Generated by Catalyst
Changes will be overwritten on next generation

USAGE:
    from streaming import format_sse, streaming_response, stream_tokens
    
    async def my_endpoint():
        async def generate():
            async for token in llm_stream():
                yield format_sse('token', {'text': token})
            yield format_sse('done', {'status': 'complete'})
        
        return streaming_response(generate())

SSE FORMAT:
    event: token
    data: {"text": "Hello"}
    
    event: done
    data: {"status": "complete"}

HEADERS:
    - Content-Type: text/event-stream
    - Cache-Control: no-cache
    - X-Accel-Buffering: no (prevents nginx buffering)
    - Connection: keep-alive
"""

from typing import AsyncGenerator, Any, Optional
from fastapi.responses import StreamingResponse
import orjson
import asyncio
import logging

logger = logging.getLogger(__name__)


def format_sse(event: str, data: Any) -> bytes:
    """
    Format data as Server-Sent Event (SSE) according to W3C spec.
    
    Uses orjson for fast JSON serialization (2-3x faster than stdlib json).
    Returns encoded bytes ready to yield from async generator.
    
    SSE Format:
        event: {event_type}
        data: {json_data}
        
        (blank line terminates event)
    
    Args:
        event: Event type (e.g., 'token', 'partial', 'done', 'error')
        data: Data to serialize (must be JSON-serializable)
    
    Returns:
        Formatted SSE event as bytes
    
    Example:
        >>> format_sse('token', {'text': 'Hello'})
        b'event: token\\ndata: {"text":"Hello"}\\n\\n'
    
    Performance:
        - O(n) where n = data size
        - Typically <1ms for objects <1KB
        - orjson is 2-3x faster than json.dumps
    
    Raises:
        TypeError: If data cannot be serialized to JSON
    """
    try:
        # Serialize data to JSON using orjson (fast C implementation)
        # orjson.dumps returns bytes directly, no need to encode
        json_data = orjson.dumps(data).decode('utf-8')
        
        # Format according to SSE spec
        # event: {type}\\n
        # data: {json}\\n
        # \\n (blank line terminates)
        sse_message = f"event: {event}\\ndata: {json_data}\\n\\n"
        
        # Return as bytes
        return sse_message.encode('utf-8')
        
    except (TypeError, ValueError) as e:
        # If serialization fails, log error and return error event
        logger.error(f"Failed to serialize data for SSE: {e}")
        error_message = f"event: error\\ndata: {{\\"error\\": \\"Serialization failed\\"}}\\n\\n"
        return error_message.encode('utf-8')


def streaming_response(
    generator: AsyncGenerator[bytes, None],
    media_type: str = "text/event-stream"
) -> StreamingResponse:
    """
    Wrap async generator in StreamingResponse with proper SSE headers.
    
    Sets critical headers to prevent buffering and enable streaming:
    - Content-Type: text/event-stream (SSE standard)
    - Cache-Control: no-cache (prevent caching of stream)
    - X-Accel-Buffering: no (prevent nginx buffering)
    - Connection: keep-alive (maintain connection)
    
    Args:
        generator: Async generator yielding SSE event bytes
        media_type: MIME type (default: text/event-stream)
    
    Returns:
        FastAPI StreamingResponse configured for SSE
    
    Example:
        @app.get("/stream")
        async def stream_endpoint():
            async def generate():
                for i in range(10):
                    yield format_sse('count', {'value': i})
                    await asyncio.sleep(0.1)
            
            return streaming_response(generate())
    
    Critical Headers:
        X-Accel-Buffering: no
            - MUST be set when using nginx reverse proxy
            - Without this, nginx buffers entire response before sending
            - Defeats purpose of streaming
    """
    return StreamingResponse(
        generator,
        media_type=media_type,
        headers={
            # Prevent any caching of streaming response
            "Cache-Control": "no-cache",
            
            # Prevent nginx from buffering the response
            # CRITICAL: Without this, nginx buffers entire stream
            "X-Accel-Buffering": "no",
            
            # Maintain connection for streaming
            "Connection": "keep-alive",
        }
    )


async def stream_tokens(
    token_generator: AsyncGenerator[str, None],
    include_done: bool = True
) -> AsyncGenerator[bytes, None]:
    """
    Stream LLM tokens as SSE events, one token at a time.
    
    Consumes an async generator of raw tokens (strings) and yields
    formatted SSE events. Optionally emits final "done" event with
    complete text and token count.
    
    This is the primary function used by LLM nodes (Anthropic, OpenAI, Groq)
    to stream responses to the client.
    
    Args:
        token_generator: Async generator yielding token strings
        include_done: Whether to emit final "done" event (default: True)
    
    Yields:
        SSE event bytes (event: token, event: done)
    
    Example:
        async def llm_stream():
            # Simulated LLM token stream
            for token in ["Hello", " ", "world", "!"]:
                yield token
                await asyncio.sleep(0.01)
        
        async for chunk in stream_tokens(llm_stream()):
            # Send chunk to client
            pass
    
    Events Emitted:
        token: { "text": "Hello" }
        token: { "text": " " }
        token: { "text": "world" }
        done: { "text": "Hello world", "token_count": 3 }
    
    Performance:
        - Yields tokens immediately as received (no buffering)
        - Minimal overhead (<1ms per token)
        - Memory usage: O(1) for streaming, O(n) if include_done=True
    
    Raises:
        Exception: Propagates exceptions from token_generator
    """
    # Accumulate full text if we need to emit "done" event
    full_text = [] if include_done else None
    token_count = 0
    
    try:
        # Stream each token as it arrives
        async for token in token_generator:
            # Count token
            token_count += 1
            
            # Accumulate if needed for final event
            if full_text is not None:
                full_text.append(token)
            
            # Emit token event immediately (no buffering)
            yield format_sse('token', {'text': token})
        
        # Emit final "done" event with complete text
        if include_done and full_text is not None:
            yield format_sse('done', {
                'text': ''.join(full_text),
                'token_count': token_count,
                'status': 'complete'
            })
    
    except Exception as e:
        # On error, emit error event
        logger.error(f"Error during token streaming: {e}")
        yield format_sse('error', {
            'error': str(e),
            'message': 'Token streaming failed'
        })
        raise


async def stream_partial_results(
    result_generators: list[AsyncGenerator[Any, None]],
    merge_results: bool = True
) -> AsyncGenerator[bytes, None]:
    """
    Stream results from multiple parallel operations as they complete.
    
    Uses asyncio.as_completed() to yield results immediately when ready,
    not waiting for all operations to finish. Useful for parallel LLM calls,
    database queries, or API requests where you want to show results
    progressively.
    
    Args:
        result_generators: List of async generators to consume in parallel
        merge_results: Whether to merge all results in final event
    
    Yields:
        SSE event bytes (event: partial, event: complete)
    
    Example:
        async def search_qdrant():
            yield {"source": "qdrant", "results": [...]}
        
        async def search_postgres():
            yield {"source": "postgres", "results": [...]}
        
        # Stream results as they complete (not all at once)
        async for chunk in stream_partial_results([
            search_qdrant(),
            search_postgres()
        ]):
            # First result arrives → sent immediately
            # Second result arrives → sent immediately
            # Don't wait for both to complete before sending
            pass
    
    Events Emitted:
        partial: { "source": "qdrant", "results": [...], "index": 0 }
        partial: { "source": "postgres", "results": [...], "index": 1 }
        complete: { "results": [...], "count": 2 }
    
    Performance:
        - Results sent as soon as ready (true parallelism)
        - No blocking waiting for slowest operation
        - Memory: O(k) where k = number of generators
    
    Use Cases:
        - Parallel vector searches across collections
        - Multiple LLM calls with different prompts
        - Fan-out API requests
        - Parallel database queries
    """
    # Collect all results if we need to merge at end
    all_results = [] if merge_results else None
    completed_count = 0
    
    try:
        # Convert generators to tasks
        # as_completed yields tasks as they finish (not in order)
        tasks = [
            asyncio.create_task(_consume_generator(gen))
            for gen in result_generators
        ]
        
        # Stream results as they complete
        for coro in asyncio.as_completed(tasks):
            try:
                # Await the completed task
                result = await coro
                completed_count += 1
                
                # Store if merging at end
                if all_results is not None:
                    all_results.append(result)
                
                # Emit partial result immediately
                yield format_sse('partial', {
                    'result': result,
                    'index': completed_count - 1,
                    'status': 'partial'
                })
            
            except Exception as e:
                # Log error but continue with other tasks
                logger.error(f"Parallel task failed: {e}")
                yield format_sse('error', {
                    'error': str(e),
                    'index': completed_count,
                    'message': 'Partial result failed'
                })
        
        # Emit final complete event
        if merge_results and all_results is not None:
            yield format_sse('complete', {
                'results': all_results,
                'count': completed_count,
                'status': 'complete'
            })
        else:
            yield format_sse('complete', {
                'count': completed_count,
                'status': 'complete'
            })
    
    except Exception as e:
        logger.error(f"Error during parallel streaming: {e}")
        yield format_sse('error', {
            'error': str(e),
            'message': 'Parallel streaming failed'
        })
        raise


async def _consume_generator(generator: AsyncGenerator[Any, None]) -> Any:
    """
    Helper to consume async generator and return last value.
    
    Used internally by stream_partial_results() to convert generators
    to awaitables for asyncio.as_completed().
    
    Args:
        generator: Async generator to consume
    
    Returns:
        Last value yielded by generator
    """
    result = None
    async for value in generator:
        result = value
    return result
`;
}

/**
 * Generate requirements.txt entry for streaming dependencies
 * 
 * @returns Python package requirements
 */
export function getStreamingDependencies(): string[] {
  return [
    'fastapi>=0.104.0',
    'orjson>=3.9.0',  // Fast JSON serialization
  ];
}

/**
 * Generate usage example for documentation
 * 
 * @returns Python code example
 */
export function generateStreamingExample(): string {
  return `"""
Example: Streaming LLM tokens with Server-Sent Events

This example shows how to use the streaming utilities to stream
LLM responses to clients in real-time.
"""

from fastapi import FastAPI
from streaming import format_sse, streaming_response, stream_tokens

app = FastAPI()


@app.post("/api/llm/stream")
async def stream_llm_response():
    """Stream LLM tokens as they are generated."""
    
    async def llm_token_generator():
        """Simulated LLM token stream."""
        tokens = ["Hello", " ", "from", " ", "Claude", "!"]
        for token in tokens:
            await asyncio.sleep(0.05)  # Simulate generation delay
            yield token
    
    # Use stream_tokens to format as SSE
    async def generate():
        async for chunk in stream_tokens(llm_token_generator()):
            yield chunk
    
    return streaming_response(generate())


@app.get("/api/search/parallel")
async def parallel_search():
    """Stream results from parallel searches as they complete."""
    
    async def search_source_1():
        await asyncio.sleep(0.1)
        yield {"source": "qdrant", "results": [1, 2, 3]}
    
    async def search_source_2():
        await asyncio.sleep(0.2)
        yield {"source": "postgres", "results": [4, 5, 6]}
    
    # Stream results as they complete (not all at once)
    async def generate():
        async for chunk in stream_partial_results([
            search_source_1(),
            search_source_2()
        ]):
            yield chunk
    
    return streaming_response(generate())
`;
}
