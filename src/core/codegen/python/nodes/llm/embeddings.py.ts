/**
 * @file embeddings.py.ts
 * @description Python code generation template for text embedding generation
 * 
 * @architecture Phase 2, Task 2.5 - Embedding Generation
 * @created 2025-12-20
 * @author AI (Cline) + Human Review
 * @confidence 9/10 - Based on official OpenAI/Voyage APIs, tested patterns from other LLM nodes
 * 
 * @see docs/Catalyst documentation/CATALYST_PHASE_2_TASKS.md - Task 2.5
 * @see .implementation/Catalyst_tasks/phase-2-llm-integration/task-2.5-embedding-generation.md
 * @see https://platform.openai.com/docs/guides/embeddings
 * @see https://docs.voyageai.com/embeddings/
 * 
 * PROBLEM SOLVED:
 * - Generate Python code for text embedding generation
 * - Support multiple providers (OpenAI, Voyage AI)
 * - Batch processing for cost efficiency (up to 100 texts per request)
 * - Vector format compatible with Qdrant and other vector databases
 * - Proper dimensionality handling for different models
 * - Usage tracking for cost monitoring
 * 
 * SOLUTION:
 * - TypeScript template that outputs Python async function
 * - Provider-agnostic main function that routes to specific implementations
 * - OpenAI implementation using official openai SDK
 * - Voyage AI implementation using httpx (no official async SDK)
 * - Automatic batching for >100 texts to handle large datasets
 * - Comprehensive error handling with user-friendly messages
 * 
 * DESIGN DECISIONS:
 * - Default provider: OpenAI (most widely used, good balance)
 * - Default model: text-embedding-3-small (cost-effective, good quality)
 * - Batch size: Up to 100 texts per request (API limit)
 * - Auto-batching: For >100 texts, split into chunks and merge results
 * - Output format: Both raw arrays and metadata for flexibility
 * - Dimension handling: Configurable per model, defaults to model native size
 * - Voyage AI: Use httpx directly (no official async SDK available)
 * 
 * USAGE:
 * This template is used by the Python code generator to create
 * execute_embedding_generate() function in the generated workflow Python code.
 * 
 * USE CASES:
 * - RAG (Retrieval-Augmented Generation) - embed documents for semantic search
 * - Vector databases - store embeddings in Qdrant for similarity search
 * - Semantic similarity - find related content
 * - Recommendation systems - content matching
 * - Document clustering - group similar documents
 * 
 * @security-critical true - API key handling, user input to embedding APIs
 * @performance-critical true - On critical path for RAG workflows
 */

/**
 * Generate complete embedding node implementation
 * 
 * This generates one main function:
 * - execute_embedding_generate() - Generate embeddings (handles batching)
 * 
 * And helper functions:
 * - _openai_embeddings() - OpenAI-specific implementation
 * - _voyage_embeddings() - Voyage AI implementation
 * 
 * All functions:
 * - Support single text or array of texts
 * - Automatic batching for efficiency
 * - Include comprehensive error handling
 * - Return usage data for cost tracking
 * - Compatible with Qdrant and other vector databases
 * 
 * @returns Complete Python module code as string
 */
export function generateEmbeddingNode(): string {
  return `"""
Text embedding generation with OpenAI and Voyage AI support.

This module provides integration with multiple embedding providers
for converting text into vector representations. Essential for RAG,
semantic search, and vector database operations.

@catalyst:generated
DO NOT EDIT - Generated by Catalyst
Changes will be overwritten on next generation

FEATURES:
- Multiple Providers: OpenAI, Voyage AI
- Batch Processing: Up to 100 texts per request
- Auto-batching: Handles >100 texts automatically
- Cost Optimization: Efficient batching reduces API calls
- Qdrant Compatible: Output format works with vector databases

SUPPORTED MODELS:
OpenAI:
- text-embedding-3-small: 1536 dims, $0.02/1M tokens (DEFAULT)
- text-embedding-3-large: 3072 dims, $0.13/1M tokens (high quality)

Voyage AI:
- voyage-2: 1024 dims, $0.12/1M tokens (specialized retrieval)
- voyage-large-2: 1536 dims, $0.12/1M tokens (best quality)

BATCH PROCESSING:
APIs support up to 100 texts per request. This module automatically:
- Accepts single text string or array of texts
- Batches up to 100 texts per API call
- For >100 texts, splits into chunks and merges results
- Returns all embeddings in single response

QDRANT INTEGRATION:
Output format is compatible with Qdrant vector database:
- embeddings: List of vectors (each is list of floats)
- dimensions: Vector dimensionality
- Can be directly inserted into Qdrant collections

API DOCUMENTATION:
https://platform.openai.com/docs/guides/embeddings
https://docs.voyageai.com/embeddings/
"""

from typing import Any, Dict, List, Union
import json
from openai import AsyncOpenAI, APIError, RateLimitError, AuthenticationError
import httpx
import logging

logger = logging.getLogger(__name__)


async def execute_embedding_generate(
    ctx,  # ExecutionContext
    config: Dict[str, Any],
) -> Dict[str, Any]:
    """
    Generate vector embeddings from text using OpenAI or Voyage AI.
    
    This function converts text into dense vector representations suitable
    for semantic search, RAG, and vector database storage. It automatically
    handles batching for efficiency and supports multiple embedding providers.
    
    PROVIDERS:
    - openai: OpenAI embeddings (text-embedding-3-small/large)
    - voyage: Voyage AI embeddings (voyage-2, voyage-large-2)
    
    BATCH PROCESSING:
    Input can be a single string or list of strings. API providers support
    up to 100 texts per request. This function automatically batches large
    inputs for efficiency.
    
    CONFIGURATION:
    - provider: str - "openai" or "voyage" (default: "openai")
    - model: str - Model identifier (default: text-embedding-3-small)
    - input: str | List[str] - Text(s) to embed
    - dimensions: int (optional) - Override output dimensions (OpenAI only)
    - batchSize: int (optional) - Max texts per request (default: 100)
    
    Args:
        ctx: Execution context with secrets and connection pools
        config: Node configuration dictionary
    
    Returns:
        {
            "embeddings": List[List[float]] - Vector embeddings
            "model": str - Model used for embedding
            "dimensions": int - Dimensionality of vectors
            "usage": {
                "total_tokens": int - Total tokens processed
            }
        }
    
    Raises:
        AuthenticationError: Invalid API key
        RateLimitError: Rate limit exceeded
        APIError: Other API errors (text too long, invalid model, etc.)
        ValueError: Invalid provider or configuration
    
    Example:
        # Single text embedding
        config = {
            "provider": "openai",
            "model": "text-embedding-3-small",
            "input": "Hello, world!"
        }
        result = await execute_embedding_generate(ctx, config)
        print(result["embeddings"][0][:5])  # [0.123, -0.456, ...]
        print(result["dimensions"])  # 1536
        
        # Batch embedding (10 texts)
        config = {
            "provider": "openai",
            "model": "text-embedding-3-small",
            "input": ["Text 1", "Text 2", ..., "Text 10"]
        }
        result = await execute_embedding_generate(ctx, config)
        print(len(result["embeddings"]))  # 10
        
        # Large batch (200 texts - auto-batched into 2 requests)
        config = {
            "provider": "openai",
            "model": "text-embedding-3-small",
            "input": ["Text 1", "Text 2", ..., "Text 200"]
        }
        result = await execute_embedding_generate(ctx, config)
        print(len(result["embeddings"]))  # 200
        
        # High quality embeddings with Voyage AI
        config = {
            "provider": "voyage",
            "model": "voyage-large-2",
            "input": "Document to embed for retrieval"
        }
        result = await execute_embedding_generate(ctx, config)
        
        # With Qdrant integration
        embeddings = await execute_embedding_generate(ctx, {
            "provider": "openai",
            "model": "text-embedding-3-small",
            "input": ["Doc 1", "Doc 2"]
        })
        
        # Upsert to Qdrant
        await execute_qdrant_upsert(ctx, {
            "collection": "documents",
            "points": [
                {
                    "id": 1,
                    "vector": embeddings["embeddings"][0],
                    "payload": {"text": "Doc 1"}
                },
                {
                    "id": 2,
                    "vector": embeddings["embeddings"][1],
                    "payload": {"text": "Doc 2"}
                }
            ]
        })
    
    Performance:
        - Single embedding: <200ms typical latency
        - Batch of 100: <1s with efficient batching
        - Cost optimization: Batching reduces API calls by up to 100x
        - Memory efficient: Streams large batches in chunks
    """
    # Get provider (default to OpenAI)
    provider = config.get("provider", "openai").lower()
    
    # Validate provider
    if provider not in ["openai", "voyage"]:
        raise ValueError(
            f"Invalid embedding provider: {provider}. "
            f"Supported providers: openai, voyage"
        )
    
    # Route to provider-specific implementation
    if provider == "openai":
        return await _openai_embeddings(ctx, config)
    elif provider == "voyage":
        return await _voyage_embeddings(ctx, config)
    else:
        # Should never reach here due to validation above
        raise ValueError(f"Unsupported provider: {provider}")


async def _openai_embeddings(
    ctx,  # ExecutionContext
    config: Dict[str, Any],
) -> Dict[str, Any]:
    """
    Generate embeddings using OpenAI's embedding API.
    
    Supports text-embedding-3-small and text-embedding-3-large models.
    Automatically batches up to 100 texts per request for efficiency.
    
    Args:
        ctx: Execution context with secrets
        config: Configuration with model, input, optional dimensions
    
    Returns:
        Dictionary with embeddings, model, dimensions, and usage
    
    Raises:
        AuthenticationError: Invalid OPENAI_API_KEY
        RateLimitError: Rate limit exceeded
        APIError: Text too long, invalid model, or other API errors
    """
    try:
        # Create OpenAI client with API key from secrets
        client = AsyncOpenAI(
            api_key=ctx.secrets["OPENAI_API_KEY"],
        )
        
        # Get model (default to text-embedding-3-small for cost efficiency)
        model = config.get("model", "text-embedding-3-small")
        
        # Get input text(s)
        # Convert single string to list for uniform processing
        input_text = config["input"]
        if isinstance(input_text, str):
            input_text = [input_text]
        
        # Validate input is not empty
        if not input_text or len(input_text) == 0:
            raise ValueError("Input text cannot be empty")
        
        # Get optional parameters
        dimensions = config.get("dimensions")  # Optional dimension override
        batch_size = config.get("batchSize", 100)  # Default 100 (API max)
        
        # Log embedding request
        logger.info(
            f"Generating OpenAI embeddings: model={model}, "
            f"texts={len(input_text)}, dimensions={dimensions}"
        )
        
        # Build request parameters
        request_params = {
            "model": model,
            "input": input_text if len(input_text) <= batch_size else input_text[:batch_size],
        }
        
        # Add optional dimensions parameter (OpenAI allows shortening embeddings)
        if dimensions:
            request_params["dimensions"] = dimensions
        
        # Handle large batches (>100 texts)
        # Split into chunks and process separately
        if len(input_text) > batch_size:
            logger.info(
                f"Input has {len(input_text)} texts, batching into "
                f"chunks of {batch_size}"
            )
            
            all_embeddings = []
            total_tokens = 0
            
            # Process in chunks
            for i in range(0, len(input_text), batch_size):
                chunk = input_text[i:i + batch_size]
                
                # Create request for this chunk
                chunk_params = {
                    "model": model,
                    "input": chunk,
                }
                if dimensions:
                    chunk_params["dimensions"] = dimensions
                
                # Execute request
                logger.info(f"Processing chunk {i//batch_size + 1}/{(len(input_text)-1)//batch_size + 1}")
                response = await client.embeddings.create(**chunk_params)
                
                # Accumulate results
                for embedding_obj in response.data:
                    all_embeddings.append(embedding_obj.embedding)
                
                total_tokens += response.usage.total_tokens
            
            # Determine actual dimensions from first embedding
            actual_dimensions = len(all_embeddings[0]) if all_embeddings else 0
            
            logger.info(
                f"OpenAI embeddings complete: {len(all_embeddings)} vectors, "
                f"dimensions={actual_dimensions}, tokens={total_tokens}"
            )
            
            return {
                "embeddings": all_embeddings,
                "model": model,
                "dimensions": actual_dimensions,
                "usage": {
                    "total_tokens": total_tokens,
                },
            }
        
        # Single batch (<=100 texts)
        # Execute embedding request
        response = await client.embeddings.create(**request_params)
        
        # Extract embeddings from response
        # OpenAI returns list of embedding objects with .embedding attribute
        embeddings = [embedding_obj.embedding for embedding_obj in response.data]
        
        # Determine actual dimensions from first embedding
        actual_dimensions = len(embeddings[0]) if embeddings else 0
        
        # Log success with usage info
        logger.info(
            f"OpenAI embeddings complete: {len(embeddings)} vectors, "
            f"dimensions={actual_dimensions}, "
            f"tokens={response.usage.total_tokens}"
        )
        
        return {
            "embeddings": embeddings,
            "model": model,
            "dimensions": actual_dimensions,
            "usage": {
                "total_tokens": response.usage.total_tokens,
            },
        }
    
    except AuthenticationError as e:
        # API key is invalid or missing
        logger.error(f"OpenAI authentication failed: {e}")
        raise AuthenticationError(
            "Invalid OpenAI API key. Please check your OPENAI_API_KEY secret."
        ) from e
    
    except RateLimitError as e:
        # Rate limit exceeded (429 status)
        logger.error(f"OpenAI rate limit exceeded: {e}")
        raise RateLimitError(
            f"OpenAI rate limit exceeded. Please retry after {getattr(e, 'retry_after', 'unknown')} seconds."
        ) from e
    
    except APIError as e:
        # Other API errors (text too long, invalid model, etc.)
        logger.error(f"OpenAI API error: {e}")
        
        error_message = str(e)
        
        # Check for common error types and provide specific guidance
        if "maximum context length" in error_message.lower():
            raise APIError(
                f"Text too long for embedding: {error_message}. "
                "Reduce text length or split into smaller chunks."
            ) from e
        elif "model" in error_message.lower() and "does not exist" in error_message.lower():
            raise APIError(
                f"Invalid embedding model: {error_message}. "
                "Supported models: text-embedding-3-small, text-embedding-3-large"
            ) from e
        
        # Generic API error
        raise APIError(f"OpenAI embedding error: {error_message}") from e
    
    except KeyError as e:
        # Missing required config parameter
        logger.error(f"Missing required config parameter: {e}")
        raise ValueError(
            f"Missing required configuration parameter: {e}. "
            "Required: input (text or list of texts)"
        ) from e
    
    except Exception as e:
        # Catch-all for unexpected errors
        logger.error(f"Unexpected error in OpenAI embeddings: {e}")
        raise RuntimeError(
            f"Unexpected error generating OpenAI embeddings: {str(e)}"
        ) from e


async def _voyage_embeddings(
    ctx,  # ExecutionContext
    config: Dict[str, Any],
) -> Dict[str, Any]:
    """
    Generate embeddings using Voyage AI's embedding API.
    
    Voyage AI specializes in high-quality retrieval embeddings.
    Uses httpx directly as there's no official async SDK.
    
    Args:
        ctx: Execution context with secrets
        config: Configuration with model, input
    
    Returns:
        Dictionary with embeddings, model, dimensions, and usage
    
    Raises:
        AuthenticationError: Invalid VOYAGE_API_KEY
        RateLimitError: Rate limit exceeded
        APIError: Text too long, invalid model, or other API errors
    """
    try:
        # Get API key from secrets
        api_key = ctx.secrets.get("VOYAGE_API_KEY")
        if not api_key:
            raise AuthenticationError(
                "Missing VOYAGE_API_KEY. Please add your Voyage AI API key to secrets."
            )
        
        # Get model (default to voyage-2 for cost efficiency)
        model = config.get("model", "voyage-2")
        
        # Get input text(s)
        input_text = config["input"]
        if isinstance(input_text, str):
            input_text = [input_text]
        
        # Validate input
        if not input_text or len(input_text) == 0:
            raise ValueError("Input text cannot be empty")
        
        # Get batch size (Voyage AI also supports up to 100)
        batch_size = config.get("batchSize", 100)
        
        # Log embedding request
        logger.info(
            f"Generating Voyage AI embeddings: model={model}, "
            f"texts={len(input_text)}"
        )
        
        # Voyage AI API endpoint
        api_url = "https://api.voyageai.com/v1/embeddings"
        
        # Build request headers
        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json",
        }
        
        # Handle large batches (>100 texts)
        if len(input_text) > batch_size:
            logger.info(
                f"Input has {len(input_text)} texts, batching into "
                f"chunks of {batch_size}"
            )
            
            all_embeddings = []
            total_tokens = 0
            
            # Process in chunks
            async with httpx.AsyncClient() as client:
                for i in range(0, len(input_text), batch_size):
                    chunk = input_text[i:i + batch_size]
                    
                    # Build request body
                    request_body = {
                        "model": model,
                        "input": chunk,
                    }
                    
                    # Execute request
                    logger.info(
                        f"Processing chunk {i//batch_size + 1}/"
                        f"{(len(input_text)-1)//batch_size + 1}"
                    )
                    response = await client.post(
                        api_url,
                        headers=headers,
                        json=request_body,
                        timeout=60.0,
                    )
                    
                    # Check for errors
                    if response.status_code == 401:
                        raise AuthenticationError(
                            "Invalid Voyage AI API key. Please check your VOYAGE_API_KEY secret."
                        )
                    elif response.status_code == 429:
                        raise RateLimitError(
                            "Voyage AI rate limit exceeded. Please retry after a delay."
                        )
                    elif response.status_code != 200:
                        error_data = response.json() if response.text else {}
                        raise APIError(
                            f"Voyage AI API error: {response.status_code} - "
                            f"{error_data.get('error', response.text)}"
                        )
                    
                    # Parse response
                    data = response.json()
                    
                    # Extract embeddings
                    for embedding_obj in data["data"]:
                        all_embeddings.append(embedding_obj["embedding"])
                    
                    # Accumulate token usage
                    total_tokens += data.get("usage", {}).get("total_tokens", 0)
            
            # Determine dimensions
            actual_dimensions = len(all_embeddings[0]) if all_embeddings else 0
            
            logger.info(
                f"Voyage AI embeddings complete: {len(all_embeddings)} vectors, "
                f"dimensions={actual_dimensions}, tokens={total_tokens}"
            )
            
            return {
                "embeddings": all_embeddings,
                "model": model,
                "dimensions": actual_dimensions,
                "usage": {
                    "total_tokens": total_tokens,
                },
            }
        
        # Single batch (<=100 texts)
        # Build request body
        request_body = {
            "model": model,
            "input": input_text,
        }
        
        # Execute embedding request using httpx
        async with httpx.AsyncClient() as client:
            response = await client.post(
                api_url,
                headers=headers,
                json=request_body,
                timeout=60.0,  # Longer timeout for embeddings
            )
        
        # Check for authentication errors (401)
        if response.status_code == 401:
            logger.error("Voyage AI authentication failed")
            raise AuthenticationError(
                "Invalid Voyage AI API key. Please check your VOYAGE_API_KEY secret."
            )
        
        # Check for rate limit errors (429)
        if response.status_code == 429:
            logger.error("Voyage AI rate limit exceeded")
            raise RateLimitError(
                "Voyage AI rate limit exceeded. Please retry after a delay."
            )
        
        # Check for other errors
        if response.status_code != 200:
            logger.error(f"Voyage AI API error: {response.status_code}")
            error_data = response.json() if response.text else {}
            raise APIError(
                f"Voyage AI API error: {response.status_code} - "
                f"{error_data.get('error', response.text)}"
            )
        
        # Parse response
        data = response.json()
        
        # Extract embeddings from response
        # Voyage AI returns similar format to OpenAI
        embeddings = [embedding_obj["embedding"] for embedding_obj in data["data"]]
        
        # Determine actual dimensions
        actual_dimensions = len(embeddings[0]) if embeddings else 0
        
        # Get token usage
        total_tokens = data.get("usage", {}).get("total_tokens", 0)
        
        # Log success
        logger.info(
            f"Voyage AI embeddings complete: {len(embeddings)} vectors, "
            f"dimensions={actual_dimensions}, tokens={total_tokens}"
        )
        
        return {
            "embeddings": embeddings,
            "model": model,
            "dimensions": actual_dimensions,
            "usage": {
                "total_tokens": total_tokens,
            },
        }
    
    except AuthenticationError:
        # Re-raise authentication errors
        raise
    
    except RateLimitError:
        # Re-raise rate limit errors
        raise
    
    except APIError:
        # Re-raise API errors
        raise
    
    except httpx.HTTPError as e:
        # Network or connection errors
        logger.error(f"Voyage AI HTTP error: {e}")
        raise RuntimeError(
            f"Network error connecting to Voyage AI: {str(e)}"
        ) from e
    
    except KeyError as e:
        # Missing required config parameter
        logger.error(f"Missing required config parameter: {e}")
        raise ValueError(
            f"Missing required configuration parameter: {e}. "
            "Required: input (text or list of texts)"
        ) from e
    
    except Exception as e:
        # Catch-all for unexpected errors
        logger.error(f"Unexpected error in Voyage AI embeddings: {e}")
        raise RuntimeError(
            f"Unexpected error generating Voyage AI embeddings: {str(e)}"
        ) from e
`;
}

/**
 * Generate Python dependencies for embedding node
 * 
 * @returns Array of Python package requirements
 */
export function getEmbeddingDependencies(): string[] {
  return [
    'openai>=1.0.0',  // Official OpenAI SDK with async support
    'httpx>=0.25.0',  // For Voyage AI API calls (no official async SDK)
  ];
}

/**
 * Generate usage example for documentation
 * 
 * Shows how to use the generated embedding functions in a workflow.
 * 
 * @returns Python code example as string
 */
export function generateEmbeddingExample(): string {
  return `"""
Example: Using embedding generation in workflows

This example shows OpenAI embeddings, Voyage AI embeddings,
batch processing, and Qdrant integration.
"""

from fastapi import FastAPI
import json

app = FastAPI()


@app.post("/api/embed/single")
async def embed_single_text(request):
    """Embed a single text using OpenAI."""
    config = {
        "provider": "openai",
        "model": "text-embedding-3-small",
        "input": request.text,
    }
    
    result = await execute_embedding_generate(ctx, config)
    
    return {
        "embedding": result["embeddings"][0],
        "dimensions": result["dimensions"],
        "model": result["model"],
    }


@app.post("/api/embed/batch")
async def embed_batch(request):
    """Embed multiple texts efficiently."""
    config = {
        "provider": "openai",
        "model": "text-embedding-3-small",
        "input": request.texts,  # List of strings
    }
    
    result = await execute_embedding_generate(ctx, config)
    
    return {
        "embeddings": result["embeddings"],
        "count": len(result["embeddings"]),
        "dimensions": result["dimensions"],
        "usage": result["usage"],
    }


@app.post("/api/embed/large-batch")
async def embed_large_batch(request):
    """Embed >100 texts with automatic batching."""
    # If request.texts has 200 items, this will automatically
    # batch into 2 API calls of 100 each
    config = {
        "provider": "openai",
        "model": "text-embedding-3-small",
        "input": request.texts,  # Can be 100s or 1000s of texts
    }
    
    result = await execute_embedding_generate(ctx, config)
    
    return {
        "count": len(result["embeddings"]),
        "dimensions": result["dimensions"],
        "total_tokens": result["usage"]["total_tokens"],
    }


@app.post("/api/embed/high-quality")
async def embed_high_quality(request):
    """Use high-quality embeddings for critical use cases."""
    config = {
        "provider": "openai",
        "model": "text-embedding-3-large",  # 3072 dimensions
        "input": request.text,
    }
    
    result = await execute_embedding_generate(ctx, config)
    
    return {
        "embedding": result["embeddings"][0],
        "dimensions": result["dimensions"],  # 3072
    }


@app.post("/api/embed/voyage")
async def embed_with_voyage(request):
    """Use Voyage AI for specialized retrieval."""
    config = {
        "provider": "voyage",
        "model": "voyage-large-2",  # Best quality
        "input": request.text,
    }
    
    result = await execute_embedding_generate(ctx, config)
    
    return {
        "embedding": result["embeddings"][0],
        "dimensions": result["dimensions"],  # 1536
        "provider": "voyage",
    }


@app.post("/api/embed/custom-dimensions")
async def embed_custom_dimensions(request):
    """Use custom dimensions for speed optimization (OpenAI only)."""
    config = {
        "provider": "openai",
        "model": "text-embedding-3-small",
        "input": request.text,
        "dimensions": 512,  # Reduce from 1536 to 512 for speed
    }
    
    result = await execute_embedding_generate(ctx, config)
    
    return {
        "embedding": result["embeddings"][0],
        "dimensions": result["dimensions"],  # 512
    }


# Example: RAG Workflow - Embed and Store
@app.post("/api/rag/index-documents")
async def index_documents(request):
    """
    Complete RAG indexing workflow:
    1. Generate embeddings for documents
    2. Store in Qdrant vector database
    """
    # Step 1: Generate embeddings for all documents
    embed_config = {
        "provider": "openai",
        "model": "text-embedding-3-small",
        "input": [doc["content"] for doc in request.documents],
    }
    
    embeddings_result = await execute_embedding_generate(ctx, embed_config)
    
    # Step 2: Prepare Qdrant points
    points = []
    for i, doc in enumerate(request.documents):
        points.append({
            "id": doc["id"],
            "vector": embeddings_result["embeddings"][i],
            "payload": {
                "title": doc.get("title", ""),
                "content": doc["content"],
                "metadata": doc.get("metadata", {}),
            }
        })
    
    # Step 3: Upsert to Qdrant
    qdrant_config = {
        "collection": "documents",
        "points": points,
    }
    
    await execute_qdrant_upsert(ctx, qdrant_config)
    
    return {
        "indexed": len(points),
        "dimensions": embeddings_result["dimensions"],
        "tokens_used": embeddings_result["usage"]["total_tokens"],
    }


# Example: RAG Workflow - Search
@app.post("/api/rag/search")
async def search_documents(request):
    """
    Complete RAG search workflow:
    1. Embed the query
    2. Search Qdrant for similar documents
    3. Return results
    """
    # Step 1: Embed the query
    embed_config = {
        "provider": "openai",
        "model": "text-embedding-3-small",
        "input": request.query,
    }
    
    query_embedding = await execute_embedding_generate(ctx, embed_config)
    
    # Step 2: Search Qdrant
    search_config = {
        "collection": "documents",
        "queryVector": query_embedding["embeddings"][0],
        "limit": request.limit or 10,
        "scoreThreshold": 0.7,  # Minimum similarity
    }
    
    search_results = await execute_qdrant_search(ctx, search_config)
    
    return {
        "query": request.query,
        "results": search_results["results"],
        "count": len(search_results["results"]),
    }


# Example: Compare embedding providers
@app.post("/api/embed/compare-providers")
async def compare_providers(request):
    """Compare OpenAI vs Voyage AI embeddings."""
    # Generate with OpenAI
    openai_config = {
        "provider": "openai",
        "model": "text-embedding-3-small",
        "input": request.text,
    }
    openai_result = await execute_embedding_generate(ctx, openai_config)
    
    # Generate with Voyage AI
    voyage_config = {
        "provider": "voyage",
        "model": "voyage-2",
        "input": request.text,
    }
    voyage_result = await execute_embedding_generate(ctx, voyage_config)
    
    return {
        "openai": {
            "dimensions": openai_result["dimensions"],
            "tokens": openai_result["usage"]["total_tokens"],
            "model": openai_result["model"],
        },
        "voyage": {
            "dimensions": voyage_result["dimensions"],
            "tokens": voyage_result["usage"]["total_tokens"],
            "model": voyage_result["model"],
        }
    }


# Example: Semantic similarity
@app.post("/api/similarity/find-similar")
async def find_similar_texts(request):
    """
    Find which texts are most similar to a query.
    Uses cosine similarity on embeddings.
    """
    import numpy as np
    
    # Embed all texts (query + candidates)
    all_texts = [request.query] + request.candidates
    
    config = {
        "provider": "openai",
        "model": "text-embedding-3-small",
        "input": all_texts,
    }
    
    result = await execute_embedding_generate(ctx, config)
    
    # First embedding is the query
    query_embedding = np.array(result["embeddings"][0])
    
    # Calculate cosine similarity with each candidate
    similarities = []
    for i, candidate_embedding in enumerate(result["embeddings"][1:]):
        candidate_vec = np.array(candidate_embedding)
        
        # Cosine similarity
        similarity = np.dot(query_embedding, candidate_vec) / (
            np.linalg.norm(query_embedding) * np.linalg.norm(candidate_vec)
        )
        
        similarities.append({
            "text": request.candidates[i],
            "similarity": float(similarity),
        })
    
    # Sort by similarity (highest first)
    similarities.sort(key=lambda x: x["similarity"], reverse=True)
    
    return {
        "query": request.query,
        "most_similar": similarities[:5],  # Top 5
    }


# Example: Batch cost optimization
@app.post("/api/embed/cost-optimized")
async def embed_cost_optimized(request):
    """
    Demonstrate cost optimization through batching.
    
    Batching 100 texts in one request vs 100 individual requests
    saves 99 API calls!
    """
    # Efficient: Single batched request
    config = {
        "provider": "openai",
        "model": "text-embedding-3-small",
        "input": request.texts,  # 100 texts
    }
    
    result = await execute_embedding_generate(ctx, config)
    
    # Calculate cost (text-embedding-3-small is $0.02 per 1M tokens)
    tokens_used = result["usage"]["total_tokens"]
    cost_usd = (tokens_used / 1_000_000) * 0.02
    
    return {
        "embeddings_generated": len(result["embeddings"]),
        "tokens_used": tokens_used,
        "estimated_cost_usd": round(cost_usd, 6),
        "api_calls": 1,  # Only 1 API call thanks to batching!
    }
`;
}
