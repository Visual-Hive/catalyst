/**
 * @file router.py.ts
 * @description Python code generation template for LLM Router node with intelligent provider selection
 * 
 * @architecture Phase 2, Task 2.7 - LLM Router
 * @created 2025-12-20
 * @author AI (Cline) + Human Review
 * @confidence 9/10 - Config-driven routing with preset strategies, well-tested pattern
 * 
 * @see docs/Catalyst documentation/CATALYST_PHASE_2_TASKS.md - Task 2.7
 * @see .implementation/Catalyst_tasks/phase-2-llm-integration/task-2.7-llm-router.md
 * 
 * PROBLEM SOLVED:
 * - Need intelligent routing between LLM providers (Anthropic, OpenAI, Groq)
 * - Different use cases require different optimization (cost, speed, quality)
 * - Hardcoding provider choices prevents flexibility and optimization
 * - Manual provider selection is error-prone and inflexible
 * 
 * SOLUTION:
 * - Config-driven routing system with preset strategies
 * - Cost strategy: Route to cheapest model that meets requirements
 * - Speed strategy: Route to fastest provider (Groq)
 * - Quality strategy: Route to highest quality models (Claude Opus)
 * - Custom conditions: Full flexibility via Python expressions
 * - Fallback mechanism: Always have a default provider
 * - Logging: Track all routing decisions for debugging and optimization
 * 
 * DESIGN DECISIONS:
 * - Preset strategies for common patterns (cost/speed/quality/balanced)
 * - Python expression evaluation for custom conditions
 * - First-match-wins routing (predictable, easy to reason about)
 * - Fallback required (always have a default)
 * - Route to existing provider functions (no duplication)
 * - No streaming support in router (providers handle it)
 * - Condition evaluation context: input, messages, env
 * 
 * ROUTING STRATEGIES:
 * - cost: Groq 8B → Groq 70B → Claude Sonnet (cheapest to most expensive)
 * - speed: Always Groq 70B (ultra-fast LPU inference)
 * - quality: Claude Sonnet → Claude Opus (best reasoning)
 * - balanced: Groq 70B for simple → Claude Sonnet for complex
 * - custom: User-defined condition expressions
 * 
 * USAGE:
 * This template is used by the Python code generator to create
 * execute_llm_router() function in the generated workflow Python code.
 * 
 * @security-critical false - Routes to other secure functions
 * @performance-critical true - On critical path, must be fast
 */

/**
 * Generate complete LLM Router node implementation
 * 
 * This generates the main routing function:
 * - execute_llm_router() - Evaluates conditions and routes to provider
 * 
 * The function:
 * - Supports preset strategies (cost/speed/quality/balanced)
 * - Supports custom condition expressions
 * - Routes to Anthropic, OpenAI, or Groq based on conditions
 * - Includes fallback mechanism when no conditions match
 * - Logs all routing decisions for debugging
 * - Returns same structure as individual provider functions
 * 
 * @returns Complete Python module code as string
 */
export function generateLLMRouterNode(): string {
  return `"""
LLM Router node for intelligent provider selection based on cost, speed, or quality.

This module provides intelligent routing between LLM providers (Anthropic Claude,
OpenAI GPT, Groq Llama/Mixtral) based on conditions like cost optimization, speed
requirements, quality needs, or custom business logic.

@catalyst:generated
DO NOT EDIT - Generated by Catalyst
Changes will be overwritten on next generation

FEATURES:
- Preset Strategies: cost, speed, quality, balanced routing
- Custom Conditions: Python expressions for flexible logic
- Cost Optimization: Route simple queries to cheap models, complex to expensive
- Speed Optimization: Route to Groq's ultra-fast LPU inference
- Quality Optimization: Route to Claude Opus for best reasoning
- Fallback Mechanism: Always have a default provider when conditions don't match
- Logging: Track all routing decisions for debugging and analysis
- Provider Agnostic: Works with any combination of Anthropic, OpenAI, Groq

ROUTING STRATEGIES:

**COST STRATEGY** - Minimize API costs
- Short messages (<500 chars): Groq Llama 3.1 8B (~$0.05/1M tokens)
- Medium messages (500-2000): Groq Llama 3.1 70B (~$0.59/1M tokens)
- Long messages (>2000): Claude Sonnet ($3/1M tokens)

**SPEED STRATEGY** - Minimize latency
- Always: Groq Llama 3.1 70B (500-800 tokens/sec, <100ms first token)

**QUALITY STRATEGY** - Maximize output quality
- Simple queries (<1000 chars): Claude Sonnet (excellent quality)
- Complex reasoning (>1000 chars): Claude Opus (best reasoning)

**BALANCED STRATEGY** - Balance cost/speed/quality
- Simple queries (<500 chars): Groq Llama 3.1 70B (fast, cheap)
- Complex queries (>500 chars): Claude Sonnet (quality, reasonable cost)

**CUSTOM STRATEGY** - User-defined conditions
- Full control via Python expressions
- Access to input, messages, environment variables
- Flexible business logic

USE CASES:
- Cost-conscious applications: Minimize API spend
- Real-time applications: Prioritize speed (Groq)
- High-quality outputs: Prioritize reasoning (Claude)
- Mixed workloads: Route based on request complexity
- A/B testing: Compare providers for same inputs
- Load balancing: Distribute across providers
- Fallback routing: Switch providers on rate limits

API COST COMPARISON (Approximate per 1M tokens):
| Provider  | Model              | Input  | Output |
|-----------|-------------------|--------|--------|
| Groq      | Llama 3.1 8B      | $0.05  | $0.08  |
| Groq      | Llama 3.1 70B     | $0.59  | $0.79  |
| Anthropic | Claude 3.5 Sonnet | $3.00  | $15.00 |
| Anthropic | Claude 3 Opus     | $15.00 | $75.00 |
| OpenAI    | GPT-3.5 Turbo     | $0.50  | $1.50  |
| OpenAI    | GPT-4 Turbo       | $10.00 | $30.00 |

DOCUMENTATION:
See task-2.7-llm-router.md for full documentation and examples.
"""

from typing import Any, Dict, Optional
import logging

logger = logging.getLogger(__name__)

# Import provider execution functions
# These are generated by tasks 2.2, 2.3, 2.4
from .anthropic import execute_anthropic_completion
from .openai import execute_openai_completion
from .groq import execute_groq_completion


# ============================================================
# PRESET ROUTING STRATEGIES
# ============================================================

# Preset strategies provide common routing patterns out of the box
# Users can select a strategy without writing custom conditions
STRATEGIES = {
    # COST STRATEGY: Minimize API costs
    # Route to cheapest providers first, more expensive for complex tasks
    "cost": [
        {
            "condition": "len(str(messages)) < 500",
            "provider": "groq",
            "model": "llama-3.1-8b-instant",
            "reason": "Short message - use cheapest/fastest model"
        },
        {
            "condition": "len(str(messages)) < 2000",
            "provider": "groq",
            "model": "llama-3.1-70b-versatile",
            "reason": "Medium message - use balanced Groq model"
        },
        {
            "condition": "always",
            "provider": "anthropic",
            "model": "claude-3-5-sonnet-20241022",
            "reason": "Long message - use high-quality model"
        }
    ],
    
    # SPEED STRATEGY: Minimize latency
    # Always route to Groq's ultra-fast LPU inference
    "speed": [
        {
            "condition": "always",
            "provider": "groq",
            "model": "llama-3.1-70b-versatile",
            "reason": "Groq LPU provides 500-800 tokens/sec"
        }
    ],
    
    # QUALITY STRATEGY: Maximize output quality
    # Route to best reasoning models (Claude)
    "quality": [
        {
            "condition": "len(str(messages)) < 1000",
            "provider": "anthropic",
            "model": "claude-3-5-sonnet-20241022",
            "reason": "Simple query - Sonnet provides excellent quality"
        },
        {
            "condition": "always",
            "provider": "anthropic",
            "model": "claude-3-opus-20240229",
            "reason": "Complex query - Opus provides best reasoning"
        }
    ],
    
    # BALANCED STRATEGY: Balance cost/speed/quality
    # Use fast/cheap for simple, quality for complex
    "balanced": [
        {
            "condition": "len(str(messages)) < 500",
            "provider": "groq",
            "model": "llama-3.1-70b-versatile",
            "reason": "Simple query - prioritize speed and cost"
        },
        {
            "condition": "always",
            "provider": "anthropic",
            "model": "claude-3-5-sonnet-20241022",
            "reason": "Complex query - prioritize quality"
        }
    ]
}


# ============================================================
# HELPER FUNCTIONS
# ============================================================

def _evaluate_condition(
    condition: str,
    ctx: Any,
    config: Dict[str, Any]
) -> bool:
    """
    Evaluate a routing condition expression.
    
    Conditions are Python expressions that have access to:
    - messages: The messages array being routed
    - input: The workflow input data
    - env: Environment variables
    - ctx: Full execution context
    
    Special condition "always" always returns True (fallback).
    
    Args:
        condition: Python expression string or "always"
        ctx: Execution context
        config: Router configuration (for messages access)
    
    Returns:
        True if condition matches, False otherwise
    
    Example conditions:
        - "always" - Always match (fallback)
        - "len(str(messages)) < 500" - Message length check
        - "input.get('priority') == 'high'" - Check input field
        - "env.get('USE_FAST_MODEL') == 'true'" - Environment check
    
    Security:
        Conditions are evaluated with restricted globals for safety.
        Only safe built-ins and provided context are available.
    """
    # Special case: "always" always matches (used for fallback)
    if condition == "always":
        return True
    
    # Build safe evaluation context
    # Only expose necessary variables and safe built-ins
    eval_context = {
        # Provide access to messages for length checks
        "messages": config.get("messages", []),
        
        # Provide access to workflow input (if available)
        "input": getattr(ctx, "input", {}),
        
        # Provide access to environment variables
        "env": getattr(ctx, "env", {}),
        
        # Provide full context for advanced conditions
        "ctx": ctx,
        
        # Safe built-ins for string operations
        "len": len,
        "str": str,
        "int": int,
        "float": float,
        "bool": bool,
    }
    
    try:
        # Evaluate the condition expression
        # This returns True/False based on the expression
        result = eval(condition, {"__builtins__": {}}, eval_context)
        
        # Ensure result is boolean
        return bool(result)
    
    except Exception as e:
        # If condition evaluation fails, log error and return False
        # This prevents bad conditions from breaking the router
        logger.error(
            f"Failed to evaluate routing condition '{condition}': {e}"
        )
        return False


def _build_provider_config(
    route: Dict[str, Any],
    base_config: Dict[str, Any]
) -> Dict[str, Any]:
    """
    Build provider-specific configuration from route and base config.
    
    This merges the selected route's model with the base configuration
    (messages, max_tokens, temperature, etc.) to create a complete
    config for the target provider.
    
    Args:
        route: Selected route with provider and model
        base_config: Base configuration with messages and parameters
    
    Returns:
        Complete provider configuration dict
    
    Example:
        route = {"provider": "anthropic", "model": "claude-3-5-sonnet-20241022"}
        base_config = {"messages": [...], "max_tokens": 2048}
        
        result = {
            "model": "claude-3-5-sonnet-20241022",
            "messages": [...],
            "max_tokens": 2048
        }
    """
    # Start with the selected model from route
    provider_config = {
        "model": route["model"],
    }
    
    # Add messages (required for all providers)
    provider_config["messages"] = base_config.get("messages", [])
    
    # Add optional system prompt if present
    # (Anthropic uses this, OpenAI includes in messages)
    if base_config.get("system"):
        provider_config["system"] = base_config["system"]
    
    # Add optional max_tokens if present
    if base_config.get("max_tokens"):
        provider_config["max_tokens"] = base_config["max_tokens"]
    
    # Add optional temperature if present
    if base_config.get("temperature") is not None:
        provider_config["temperature"] = base_config["temperature"]
    
    return provider_config


# ============================================================
# MAIN ROUTING FUNCTION
# ============================================================

async def execute_llm_router(
    ctx,  # ExecutionContext
    config: Dict[str, Any],
) -> Dict[str, Any]:
    """
    Route LLM request to appropriate provider based on conditions.
    
    This function evaluates routing conditions in order and selects
    the first matching provider. If no conditions match, it uses the
    fallback provider.
    
    ROUTING PROCESS:
    1. Determine routes (preset strategy or custom routes)
    2. Evaluate conditions in order (first match wins)
    3. If match found, use that provider + model
    4. If no match, use fallback provider + model
    5. Build provider config with messages and parameters
    6. Call appropriate provider function
    7. Return provider's response
    
    CONFIGURATION:
    - strategy: str (optional) - "cost", "speed", "quality", "balanced", or "custom"
    - routes: List[dict] (required if custom) - Custom routing rules
    - fallback: dict (required) - Default provider when no match
    - messages: List[dict] (required) - Messages to route
    - system: str (optional) - System prompt
    - max_tokens: int (optional) - Maximum tokens to generate
    - temperature: float (optional) - Sampling temperature
    
    Args:
        ctx: Execution context with secrets and connections
        config: Router configuration dictionary
    
    Returns:
        Same structure as individual provider functions:
        {
            "content": str - Generated text response
            "model": str - Actual model used
            "usage": dict - Token usage statistics
            "finish_reason": str - Why generation stopped
            "provider": str - Which provider was used (added by router)
        }
    
    Raises:
        ValueError: Invalid configuration or unknown provider
        RuntimeError: Routing or provider execution failed
    
    Example - Cost strategy:
        config = {
            "strategy": "cost",
            "fallback": {
                "provider": "anthropic",
                "model": "claude-3-5-sonnet-20241022"
            },
            "messages": [
                {"role": "user", "content": "What is 2+2?"}
            ]
        }
        
        result = await execute_llm_router(ctx, config)
        # Routes to Groq 8B (cheapest for short query)
        # result["provider"] == "groq"
        # result["content"] == "2+2 equals 4."
    
    Example - Custom conditions:
        config = {
            "strategy": "custom",
            "routes": [
                {
                    "condition": "input.get('priority') == 'high'",
                    "provider": "anthropic",
                    "model": "claude-3-opus-20240229"
                },
                {
                    "condition": "len(str(messages)) < 200",
                    "provider": "groq",
                    "model": "llama-3.1-8b-instant"
                }
            ],
            "fallback": {
                "provider": "anthropic",
                "model": "claude-3-5-sonnet-20241022"
            },
            "messages": [{"role": "user", "content": "..."}]
        }
        
        result = await execute_llm_router(ctx, config)
        # Evaluates conditions in order, uses first match
    
    Performance:
        - Condition evaluation: <1ms (simple expressions)
        - Routing overhead: Negligible
        - Total latency: Determined by selected provider
        - Cost: Varies by selected provider
    
    Debugging:
        All routing decisions are logged at INFO level:
        - Which strategy/conditions were evaluated
        - Which route matched (or if fallback used)
        - Which provider and model were selected
        - Review logs to optimize routing logic
    """
    try:
        # Step 1: Determine routes (preset strategy or custom)
        # If user specified a preset strategy, use it
        # Otherwise, use custom routes from config
        strategy = config.get("strategy", "custom")
        
        if strategy in STRATEGIES:
            # Use preset strategy routes
            routes = STRATEGIES[strategy]
            logger.info(f"LLM Router: Using '{strategy}' strategy")
        else:
            # Use custom routes from config
            routes = config.get("routes", [])
            logger.info(f"LLM Router: Using custom routing conditions")
        
        # Ensure we have routes to evaluate
        if not routes:
            logger.warning(
                "LLM Router: No routes defined, using fallback directly"
            )
            # Skip to fallback (no routes to evaluate)
            selected_provider = None
            selected_model = None
            selected_reason = "No routes defined"
        else:
            # Step 2: Evaluate conditions in order (first match wins)
            # This loop tries each route's condition until one matches
            selected_provider = None
            selected_model = None
            selected_reason = None
            
            for i, route in enumerate(routes):
                # Get condition from route
                condition = route.get("condition", "always")
                
                # Evaluate the condition
                if _evaluate_condition(condition, ctx, config):
                    # Condition matched! Use this route
                    selected_provider = route["provider"]
                    selected_model = route["model"]
                    selected_reason = route.get("reason", "Condition matched")
                    
                    logger.info(
                        f"LLM Router: Route {i+1} matched - "
                        f"provider={selected_provider}, "
                        f"model={selected_model}, "
                        f"reason={selected_reason}"
                    )
                    
                    # Break after first match (first-match-wins)
                    break
        
        # Step 3: Use fallback if no routes matched
        if selected_provider is None:
            # No conditions matched, use fallback
            fallback = config.get("fallback", {})
            
            if not fallback:
                # No fallback defined - this is a configuration error
                raise ValueError(
                    "LLM Router: No routes matched and no fallback defined. "
                    "Config must include a 'fallback' with provider and model."
                )
            
            selected_provider = fallback["provider"]
            selected_model = fallback["model"]
            selected_reason = "Fallback (no conditions matched)"
            
            logger.info(
                f"LLM Router: Using fallback - "
                f"provider={selected_provider}, "
                f"model={selected_model}"
            )
        
        # Step 4: Build provider configuration
        # Merge selected model with base config (messages, etc.)
        provider_config = _build_provider_config(
            route={"provider": selected_provider, "model": selected_model},
            base_config=config
        )
        
        # Step 5: Route to appropriate provider
        # Call the selected provider's execution function
        if selected_provider == "anthropic":
            logger.info("LLM Router: Routing to Anthropic Claude")
            result = await execute_anthropic_completion(ctx, provider_config)
        
        elif selected_provider == "openai":
            logger.info("LLM Router: Routing to OpenAI GPT")
            result = await execute_openai_completion(ctx, provider_config)
        
        elif selected_provider == "groq":
            logger.info("LLM Router: Routing to Groq")
            result = await execute_groq_completion(ctx, provider_config)
        
        else:
            # Unknown provider - configuration error
            raise ValueError(
                f"LLM Router: Unknown provider '{selected_provider}'. "
                f"Supported providers: anthropic, openai, groq"
            )
        
        # Step 6: Add routing metadata to result
        # Include which provider was selected for debugging and analytics
        result["provider"] = selected_provider
        result["routing_reason"] = selected_reason
        
        # Log successful routing
        logger.info(
            f"LLM Router: Successfully routed to {selected_provider}, "
            f"model={result.get('model')}, "
            f"tokens={result.get('usage', {}).get('total_tokens', 'N/A')}"
        )
        
        return result
    
    except ValueError as e:
        # Configuration errors (invalid provider, missing fallback, etc.)
        logger.error(f"LLM Router configuration error: {e}")
        raise ValueError(f"LLM Router configuration error: {str(e)}") from e
    
    except Exception as e:
        # Runtime errors (provider failures, network issues, etc.)
        logger.error(f"LLM Router execution error: {e}")
        raise RuntimeError(f"LLM Router failed: {str(e)}") from e
`;
}

/**
 * Generate Python dependencies for LLM Router node
 * 
 * Router doesn't need additional dependencies beyond what the
 * individual provider nodes already require. It just routes to them.
 * 
 * @returns Empty array (no additional dependencies)
 */
export function getLLMRouterDependencies(): string[] {
  // Router uses functions from anthropic.py, openai.py, groq.py
  // Those modules already include their dependencies
  // No additional packages needed
  return [];
}

/**
 * Generate usage example for documentation
 * 
 * Shows how to use the LLM router with different strategies and
 * custom conditions in a workflow.
 * 
 * @returns Python code example as string
 */
export function generateLLMRouterExample(): string {
  return `"""
Example: Using LLM Router for intelligent provider selection

This example shows cost optimization, speed prioritization, quality
routing, and custom condition logic.
"""

from fastapi import FastAPI
from streaming import stream_tokens, streaming_response
import json

app = FastAPI()


@app.post("/api/chat/cost-optimized")
async def cost_optimized_endpoint(request):
    """
    Route to cheapest provider based on message length.
    
    Short messages → Groq 8B (ultra cheap)
    Medium messages → Groq 70B (balanced)
    Long messages → Claude Sonnet (quality for complexity)
    """
    config = {
        "strategy": "cost",
        "fallback": {
            "provider": "anthropic",
            "model": "claude-3-5-sonnet-20241022"
        },
        "messages": [
            {"role": "user", "content": request.message}
        ],
        "max_tokens": 2048,
    }
    
    result = await execute_llm_router(ctx, config)
    
    return {
        "response": result["content"],
        "provider": result["provider"],
        "model": result["model"],
        "routing_reason": result["routing_reason"],
        "usage": result["usage"],
    }


@app.post("/api/chat/real-time")
async def realtime_endpoint(request):
    """
    Always route to Groq for ultra-fast responses.
    
    Perfect for real-time chat applications where speed matters most.
    Groq delivers 500-800 tokens/sec with <100ms first token latency.
    """
    config = {
        "strategy": "speed",
        "fallback": {
            "provider": "groq",
            "model": "llama-3.1-70b-versatile"
        },
        "messages": [
            {"role": "user", "content": request.message}
        ],
        "max_tokens": 1024,
    }
    
    result = await execute_llm_router(ctx, config)
    
    return {
        "response": result["content"],
        "provider": result["provider"],  # Always "groq"
        "elapsed_ms": "~200-500ms"  # Groq is FAST!
    }


@app.post("/api/analysis/high-quality")
async def quality_endpoint(request):
    """
    Route to best reasoning models for complex analysis.
    
    Simple queries → Claude Sonnet (excellent)
    Complex queries → Claude Opus (best reasoning)
    """
    config = {
        "strategy": "quality",
        "fallback": {
            "provider": "anthropic",
            "model": "claude-3-opus-20240229"
        },
        "messages": [
            {"role": "system", "content": "You are an expert analyst."},
            {"role": "user", "content": request.query}
        ],
        "max_tokens": 4096,
        "temperature": 0.3,  # Lower temp for more focused analysis
    }
    
    result = await execute_llm_router(ctx, config)
    
    return {
        "analysis": result["content"],
        "provider": result["provider"],
        "model": result["model"],
    }


@app.post("/api/chat/balanced")
async def balanced_endpoint(request):
    """
    Balance cost, speed, and quality.
    
    Simple queries → Groq (fast and cheap)
    Complex queries → Claude Sonnet (quality)
    """
    config = {
        "strategy": "balanced",
        "fallback": {
            "provider": "anthropic",
            "model": "claude-3-5-sonnet-20241022"
        },
        "messages": [
            {"role": "user", "content": request.message}
        ],
        "max_tokens": 2048,
    }
    
    result = await execute_llm_router(ctx, config)
    
    return {
        "response": result["content"],
        "provider": result["provider"],
        "routing_reason": result["routing_reason"],
    }


@app.post("/api/chat/custom-routing")
async def custom_routing_endpoint(request):
    """
    Custom routing logic based on business rules.
    
    This shows how to route based on:
    - Input fields (priority, user tier)
    - Message content (keywords, length)
    - Environment variables (feature flags)
    - Time of day (off-peak routing)
    """
    config = {
        "strategy": "custom",
        "routes": [
            # High-priority requests → Best quality (Claude Opus)
            {
                "condition": "input.get('priority') == 'high'",
                "provider": "anthropic",
                "model": "claude-3-opus-20240229",
                "reason": "High priority - use best model"
            },
            
            # Premium users → High quality (Claude Sonnet)
            {
                "condition": "input.get('user_tier') == 'premium'",
                "provider": "anthropic",
                "model": "claude-3-5-sonnet-20241022",
                "reason": "Premium user - use high-quality model"
            },
            
            # Short messages → Fast and cheap (Groq)
            {
                "condition": "len(str(messages)) < 200",
                "provider": "groq",
                "model": "llama-3.1-8b-instant",
                "reason": "Short message - use fastest model"
            },
            
            # Off-peak hours → Use premium models (cost optimization)
            {
                "condition": "env.get('PEAK_HOURS') == 'false'",
                "provider": "anthropic",
                "model": "claude-3-5-sonnet-20241022",
                "reason": "Off-peak - use premium model"
            },
            
            # Default → Balanced (Groq 70B)
            {
                "condition": "always",
                "provider": "groq",
                "model": "llama-3.1-70b-versatile",
                "reason": "Default - balanced performance"
            }
        ],
        "fallback": {
            "provider": "anthropic",
            "model": "claude-3-5-sonnet-20241022"
        },
        "messages": request.messages,
        "max_tokens": request.max_tokens or 2048,
    }
    
    result = await execute_llm_router(ctx, config)
    
    return {
        "response": result["content"],
        "provider": result["provider"],
        "model": result["model"],
        "routing_reason": result["routing_reason"],
        "usage": result["usage"],
    }


@app.post("/api/ab-test")
async def ab_test_endpoint(request):
    """
    A/B test different providers for the same input.
    
    Useful for comparing quality, speed, or cost across providers.
    """
    import asyncio
    
    # Test configurations for different providers
    providers = [
        {
            "name": "Anthropic Claude",
            "config": {
                "strategy": "custom",
                "routes": [{
                    "condition": "always",
                    "provider": "anthropic",
                    "model": "claude-3-5-sonnet-20241022"
                }],
                "fallback": {"provider": "anthropic", "model": "claude-3-5-sonnet-20241022"},
                "messages": request.messages,
            }
        },
        {
            "name": "OpenAI GPT-4",
            "config": {
                "strategy": "custom",
                "routes": [{
                    "condition": "always",
                    "provider": "openai",
                    "model": "gpt-4-turbo-preview"
                }],
                "fallback": {"provider": "openai", "model": "gpt-4-turbo-preview"},
                "messages": request.messages,
            }
        },
        {
            "name": "Groq Llama 3.1",
            "config": {
                "strategy": "custom",
                "routes": [{
                    "condition": "always",
                    "provider": "groq",
                    "model": "llama-3.1-70b-versatile"
                }],
                "fallback": {"provider": "groq", "model": "llama-3.1-70b-versatile"},
                "messages": request.messages,
            }
        }
    ]
    
    # Run all providers concurrently
    import time
    
    async def test_provider(prov):
        start = time.time()
        result = await execute_llm_router(ctx, prov["config"])
        elapsed = time.time() - start
        
        return {
            "provider": prov["name"],
            "response": result["content"],
            "model": result["model"],
            "elapsed_sec": elapsed,
            "tokens": result["usage"].get("total_tokens"),
        }
    
    results = await asyncio.gather(*[test_provider(p) for p in providers])
    
    return {
        "test_results": results,
        "note": "Compare quality, speed, and choose best provider for your use case"
    }


@app.post("/api/fallback-routing")
async def fallback_routing_endpoint(request):
    """
    Demonstrate fallback when no conditions match.
    
    If all condition checks fail or return False, router uses fallback.
    This ensures requests always get processed.
    """
    config = {
        "strategy": "custom",
        "routes": [
            # Impossible condition - will never match
            {
                "condition": "False",
                "provider": "groq",
                "model": "llama-3.1-8b-instant"
            },
            # Another impossible condition
            {
                "condition": "1 == 2",
                "provider": "openai",
                "model": "gpt-3.5-turbo"
            }
        ],
        "fallback": {
            "provider": "anthropic",
            "model": "claude-3-5-sonnet-20241022"
        },
        "messages": request.messages,
    }
    
    result = await execute_llm_router(ctx, config)
    
    return {
        "response": result["content"],
        "provider": result["provider"],  # Will be "anthropic" (fallback)
        "routing_reason": result["routing_reason"],  # "Fallback (no conditions matched)"
    }


# Example: Dynamic routing based on message analysis
@app.post("/api/smart-routing")
async def smart_routing_endpoint(request):
    """
    Analyze message content and route intelligently.
    
    This example shows advanced routing based on message characteristics.
    """
    message = request.message
    
    # Determine characteristics
    is_code_related = any(word in message.lower() for word in ["code", "function", "debug", "api"])
    is_creative = any(word in message.lower() for word in ["story", "poem", "creative", "imagine"])
    is_analytical = any(word in message.lower() for word in ["analyze", "compare", "evaluate", "why"])
    
    config = {
        "strategy": "custom",
        "routes": [
            # Code questions → OpenAI (good at code)
            {
                "condition": f"{is_code_related}",
                "provider": "openai",
                "model": "gpt-4-turbo-preview",
                "reason": "Code-related - use OpenAI"
            },
            
            # Analytical questions → Claude (best reasoning)
            {
                "condition": f"{is_analytical}",
                "provider": "anthropic",
                "model": "claude-3-5-sonnet-20241022",
                "reason": "Analytical - use Claude"
            },
            
            # Creative questions → Claude (creative writing)
            {
                "condition": f"{is_creative}",
                "provider": "anthropic",
                "model": "claude-3-5-sonnet-20241022",
                "reason": "Creative - use Claude"
            },
            
            # Default → Fast option (Groq)
            {
                "condition": "always",
                "provider": "groq",
                "model": "llama-3.1-70b-versatile",
                "reason": "General query - use fast model"
            }
        ],
        "fallback": {
            "provider": "groq",
            "model": "llama-3.1-70b-versatile"
        },
        "messages": [
            {"role": "user", "content": message}
        ],
    }
    
    result = await execute_llm_router(ctx, config)
    
    return {
        "response": result["content"],
        "provider": result["provider"],
        "routing_decision": {
            "is_code_related": is_code_related,
            "is_creative": is_creative,
            "is_analytical": is_analytical,
            "reason": result["routing_reason"]
        }
    }
`;
}
