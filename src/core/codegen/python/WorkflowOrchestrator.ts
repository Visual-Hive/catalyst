/**
 * @file WorkflowOrchestrator.ts
 * @description Orchestrates Python code generation from Catalyst workflows
 * 
 * @architecture Phase 2, Task 2.8 - Integration & Activation
 * @created 2025-12-20
 * @author AI (Cline) + Human Review
 * @confidence 8/10 - Core orchestration logic, needs testing with real workflows
 * 
 * @see src/core/workflow/types.ts - Workflow data structures
 * @see src/core/codegen/python/nodes/llm/ - Node-specific generators
 * @see docs/Catalyst documentation/CATALYST_PHASE_2_TASKS.md - Task 2.8
 * 
 * PROBLEM SOLVED:
 * - Need to convert visual workflow graphs to executable Python/FastAPI code
 * - Multiple node types each have their own generator
 * - Must assemble imports, library functions, workflow logic, and API endpoint
 * - Handle node connections and data flow
 * - Generate clean, readable, production-ready Python code
 * 
 * SOLUTION:
 * - Orchestrator that coordinates all node generators
 * - Topological sort of nodes to respect execution order
 * - Collect all dependencies from used nodes
 * - Assemble into complete Python file structure
 * - FastAPI endpoint wraps workflow execution
 * - Proper error handling and logging
 * 
 * DESIGN DECISIONS:
 * - One Python file per workflow (simple deployment)
 * - FastAPI for HTTP endpoint (matches Catalyst spec)
 * - Async/await throughout (modern Python)
 * - Library functions generated at top, workflow logic at bottom
 * - Environment variables for secrets (12-factor app)
 * - Execution context pattern (ctx object)
 * 
 * GENERATED FILE STRUCTURE:
 * ```python
 * """
 * Workflow: <name>
 * Generated by Catalyst
 * """
 * 
 * # Imports
 * from fastapi import FastAPI
 * from groq import AsyncGroq
 * # ...
 * 
 * # Node library functions
 * async def execute_groq_completion(ctx, config):
 *     # ...generated from groq.py.ts
 * 
 * # FastAPI setup
 * app = FastAPI()
 * 
 * # Workflow endpoint
 * @app.post("/workflow/<name>")
 * async def workflow_handler(input_data: dict):
 *     # Execute nodes in order
 *     result_1 = await execute_groq_completion(ctx, {...})
 *     result_2 = await execute_prompt_template(ctx, {...})
 *     return {"result": result_2}
 * 
 * # Main
 * if __name__ == "__main__":
 *     uvicorn.run(app)
 * ```
 * 
 * @security-critical false - generates code, doesn't execute
 * @performance-critical false - offline code generation
 */

import type {
  WorkflowDefinition,
  NodeDefinition,
  NodeType,
  EdgeDefinition,
} from '../../workflow/types';

// Import all node generators
import {
  generateGroqNode,
  getGroqDependencies,
  generateAnthropicNode,
  getAnthropicDependencies,
  generateOpenAINode,
  getOpenAIDependencies,
  generateEmbeddingNode,
  getEmbeddingDependencies,
  generatePromptTemplateNode,
  getPromptTemplateDependencies,
  generateLLMRouterNode,
  getLLMRouterDependencies,
} from './nodes/llm';

import {
  generateHttpEndpointNode,
  getHttpEndpointDependencies,
} from './nodes/triggers';

import {
  generateStreaming,
  getStreamingDeps,
} from './StreamingGenerator';
import { generateStreamingModule } from './templates/streaming.py';

// ============================================================================
// TYPES
// ============================================================================

/**
 * Result of code generation
 */
export interface GenerationResult {
  /** Generated Python code */
  code: string;
  
  /** Python package dependencies */
  dependencies: string[];
  
  /** Workflow name (for filename) */
  workflowName: string;
  
  /** Number of nodes generated */
  nodeCount: number;
  
  /** Warning messages (non-fatal issues) */
  warnings: string[];
}

/**
 * Node generator function type
 * Takes no parameters, returns Python module code
 */
type NodeGenerator = () => string;

/**
 * Dependency getter function type
 * Returns array of Python package requirements
 */
type DependencyGetter = () => string[];

/**
 * Registry mapping node types to their generators
 */
interface NodeGeneratorRegistry {
  generator: NodeGenerator;
  dependencies: DependencyGetter;
}

// ============================================================================
// NODE GENERATOR REGISTRY
// ============================================================================

/**
 * Map node types to their generator functions
 * 
 * This is the central registry that tells the orchestrator
 * how to generate code for each node type.
 * 
 * As new node types are implemented, add them here.
 */
const NODE_GENERATORS: Record<NodeType, NodeGeneratorRegistry | undefined> = {
  // ===== TRIGGERS (Phase 2.10) =====
  httpEndpoint: {
    generator: generateHttpEndpointNode,
    dependencies: getHttpEndpointDependencies,
  },
  
  // ===== LLM NODES (Phase 2) =====
  groqCompletion: {
    generator: generateGroqNode,
    dependencies: getGroqDependencies,
  },
  
  anthropicCompletion: {
    generator: generateAnthropicNode,
    dependencies: getAnthropicDependencies,
  },
  
  openaiCompletion: {
    generator: generateOpenAINode,
    dependencies: getOpenAIDependencies,
  },
  
  embeddingGenerate: {
    generator: generateEmbeddingNode,
    dependencies: getEmbeddingDependencies,
  },
  
  promptTemplate: {
    generator: generatePromptTemplateNode,
    dependencies: getPromptTemplateDependencies,
  },
  
  llmRouter: {
    generator: generateLLMRouterNode,
    dependencies: getLLMRouterDependencies,
  },
  
  azureOpenaiCompletion: undefined,
  agenticToolCall: undefined,
  
  // ===== STUB ENTRIES (Future phases) =====
  // These will be implemented in future phases
  // For now, return undefined and warn user
  
  // Triggers (Future)
  scheduledTask: undefined,
  webhookReceiver: undefined,
  subworkflowTrigger: undefined,
  websocketEndpoint: undefined,
  queueConsumer: undefined,
  
  // Data Operations (Phase 3)
  qdrantSearch: undefined,
  qdrantUpsert: undefined,
  qdrantScroll: undefined,
  qdrantPayload: undefined,
  postgresQuery: undefined,
  directusQuery: undefined,
  graphqlQuery: undefined,
  redisOperation: undefined,
  
  // HTTP Operations (Phase 3)
  httpRequest: undefined,
  gmailOperation: undefined,
  webhookSend: undefined,
  graphqlMutation: undefined,
  
  // Control Flow (Phase 3)
  condition: undefined,
  switch: undefined,
  loop: undefined,
  parallel: undefined,
  aggregate: undefined,
  retry: undefined,
  delay: undefined,
  earlyReturn: undefined,
  
  // Transform Operations (Phase 3)
  editFields: undefined,
  javascriptFunction: undefined,
  pythonFunction: undefined,
  jsonTransform: undefined,
  mapArray: undefined,
  filterArray: undefined,
  reduceArray: undefined,
  splitArray: undefined,
  
  // Streaming (Phase 3)
  streamStart: undefined,
  streamChunk: undefined,
  streamEnd: undefined,
  streamMerge: undefined,
  streamBuffer: undefined,
  
  // Utilities (Phase 4)
  cryptoGenerate: undefined,
  executionData: undefined,
  globalVariable: undefined,
  errorHandler: undefined,
  log: undefined,
  metrics: undefined,
  rateLimit: undefined,
  validate: undefined,
};

// ============================================================================
// MAIN ORCHESTRATOR
// ============================================================================

/**
 * Generate complete Python code from a Catalyst workflow
 * 
 * This is the main entry point for Python code generation.
 * It orchestrates all the individual node generators and assembles
 * them into a complete, runnable Python file.
 * 
 * PROCESS:
 * 1. Validate workflow has nodes
 * 2. Identify which node types are used
 * 3. Generate library code for each unique node type
 * 4. Collect all dependencies
 * 5. Topologically sort nodes by connections
 * 6. Generate workflow execution logic
 * 7. Assemble complete file
 * 
 * @param workflow - Workflow definition from manifest
 * @returns Generation result with code and dependencies
 * 
 * @throws Error if workflow is invalid or contains unimplemented nodes
 * 
 * @example
 * const workflow = workflowStore.getActiveWorkflow();
 * const result = generatePythonWorkflow(workflow);
 * 
 * // Write to file
 * fs.writeFileSync(
 *   `${projectPath}/.catalyst/generated/${result.workflowName}.py`,
 *   result.code
 * );
 * 
 * // Install dependencies
 * // pip install ${result.dependencies.join(' ')}
 */
export function generatePythonWorkflow(
  workflow: WorkflowDefinition
): GenerationResult {
  // Track warnings for user feedback
  const warnings: string[] = [];
  
  // --------------------------------------------------------
  // 1. VALIDATION
  // --------------------------------------------------------
  
  // Get nodes array from map
  const nodes = Object.values(workflow.nodes);
  
  if (nodes.length === 0) {
    throw new Error('Cannot generate code for empty workflow. Add nodes first.');
  }
  
  // --------------------------------------------------------
  // 2. IDENTIFY UNIQUE NODE TYPES
  // --------------------------------------------------------
  
  // Collect unique node types used in workflow
  // We only generate library code for types that are actually used
  const usedNodeTypes = new Set<NodeType>(nodes.map(node => node.type));
  
  // Check for unimplemented node types
  const unimplementedTypes: NodeType[] = [];
  usedNodeTypes.forEach(type => {
    if (!NODE_GENERATORS[type]) {
      unimplementedTypes.push(type);
    }
  });
  
  if (unimplementedTypes.length > 0) {
    throw new Error(
      `Workflow contains unimplemented node types: ${unimplementedTypes.join(', ')}. ` +
      `These nodes will be available in future phases.`
    );
  }
  
  // --------------------------------------------------------
  // 3. GENERATE LIBRARY CODE
  // --------------------------------------------------------
  
  // Generate library functions for each unique node type
  const libraryModules: string[] = [];
  const allDependencies = new Set<string>();
  
  // Add streaming infrastructure (always needed for LLM streaming)
  if (usedNodeTypes.has('groqCompletion') ||
      usedNodeTypes.has('anthropicCompletion') ||
      usedNodeTypes.has('openaiCompletion')) {
    libraryModules.push(generateStreamingModule());
    getStreamingDeps().forEach((dep: string) => allDependencies.add(dep));
  }
  
  // Generate code for each node type
  usedNodeTypes.forEach(type => {
    const registry = NODE_GENERATORS[type];
    if (registry) {
      // Generate library module
      libraryModules.push(registry.generator());
      
      // Collect dependencies
      registry.dependencies().forEach(dep => allDependencies.add(dep));
    }
  });
  
  // --------------------------------------------------------
  // 4. GENERATE WORKFLOW LOGIC
  // --------------------------------------------------------
  
  // For now, simple sequential execution
  // Future: Respect edges and do topological sort
  const workflowLogic = generateWorkflowExecutionLogic(workflow, nodes);
  
  // --------------------------------------------------------
  // 5. ASSEMBLE COMPLETE FILE
  // --------------------------------------------------------
  
  const code = assembleCompleteFile(
    workflow,
    nodes,
    libraryModules,
    workflowLogic,
    warnings
  );
  
  // --------------------------------------------------------
  // 6. RETURN RESULT
  // --------------------------------------------------------
  
  return {
    code,
    dependencies: Array.from(allDependencies),
    workflowName: sanitizeWorkflowName(workflow.name),
    nodeCount: nodes.length,
    warnings,
  };
}

// ============================================================================
// HELPER FUNCTIONS
// ============================================================================

/**
 * Check if a node type is a trigger node
 * 
 * Trigger nodes define HOW workflows are invoked (HTTP, scheduled, webhook, etc.)
 * but they don't execute as part of the workflow logic. They are metadata only.
 * 
 * The FastAPI route decorator (@app.post) handles the actual trigger behavior,
 * so trigger nodes should be skipped during workflow execution.
 * 
 * @param nodeType - Node type to check
 * @returns true if node is a trigger, false otherwise
 */
function isTriggerNode(nodeType: NodeType): boolean {
  const triggerTypes: NodeType[] = [
    'httpEndpoint',
    'scheduledTask',
    'webhookReceiver',
    'subworkflowTrigger',
    'websocketEndpoint',
    'queueConsumer',
  ];
  return triggerTypes.includes(nodeType);
}

/**
 * Generate minimal workflow endpoint for trigger-only workflows
 * 
 * Used when a workflow contains only trigger nodes (no executable nodes).
 * This is an edge case but valid - workflow just receives data and returns it.
 * 
 * @param workflow - Workflow definition
 * @returns Python code for minimal endpoint
 */
function generateMinimalWorkflowEndpoint(workflow: WorkflowDefinition): string {
  return `
@app.post("/workflow/${sanitizeWorkflowName(workflow.name)}")
async def workflow_${sanitizeWorkflowName(workflow.name)}(input_data: dict):
    """
    ${workflow.description || 'Generated workflow endpoint'}
    
    Workflow: ${workflow.name}
    Note: This workflow contains only trigger nodes (no executable logic)
    """
    # Generate execution ID
    execution_id = str(uuid.uuid4())
    started_at = datetime.utcnow().isoformat()
    
    logger.info(f"Starting workflow: ${workflow.name} (execution: {execution_id})")
    
    # Minimal execution - just return input data
    execution_data = {
        "id": execution_id,
        "workflowId": "${workflow.id}",
        "workflowName": "${workflow.name}",
        "status": "success",
        "startedAt": started_at,
        "completedAt": datetime.utcnow().isoformat(),
        "durationMs": 0,
        "trigger": {
            "type": "${workflow.trigger?.type || 'httpEndpoint'}",
            "data": input_data
        },
        "nodeExecutions": [],
        "output": input_data,
    }
    
    logger.info(f"Workflow completed (execution: {execution_id})")
    
    # Output execution markers for Catalyst local execution capture
    import json
    print("__CATALYST_EXECUTION_START__")
    print(json.dumps(execution_data))
    print("__CATALYST_EXECUTION_END__")
    
    return {
        "status": "success",
        "workflow": "${workflow.name}",
        "executionId": execution_id,
        "result": input_data,
    }
`;
}

/**
 * Generate workflow execution logic
 * 
 * Creates the FastAPI endpoint that executes the workflow nodes in order.
 * 
 * IMPORTANT: Trigger nodes are filtered out because they define invocation
 * method (HTTP, scheduled, etc.) but don't execute code themselves.
 * The FastAPI route decorator handles trigger behavior.
 * 
 * For MVP: Simple sequential execution
 * Future: Respect edges, parallel execution, conditional branches
 * 
 * @param workflow - Workflow definition
 * @param nodes - Array of nodes to execute
 * @returns Python code for workflow endpoint
 */
function generateWorkflowExecutionLogic(
  workflow: WorkflowDefinition,
  nodes: NodeDefinition[]
): string {
  // Filter out trigger nodes - they don't execute, they define invocation method
  const executableNodes = nodes.filter(node => !isTriggerNode(node.type));
  
  // If no executable nodes, return minimal endpoint
  if (executableNodes.length === 0) {
    return generateMinimalWorkflowEndpoint(workflow);
  }
  
  // Generate node execution calls with tracking
  const nodeExecutions = executableNodes.map((node, index) => {
    const varName = `result_${index + 1}`;
    const functionName = getExecutionFunctionName(node.type);
    const config = JSON.stringify(node.config, null, 4);
    
    return `        # Node: ${node.name} (${node.type})
        node_exec_${index + 1} = NodeExecution(
            node_id="${node.id}",
            node_name="${node.name}",
            node_type="${node.type}",
            status="running",
            started_at=datetime.utcnow().isoformat(),
            input={}  # TODO: Extract actual input from config
        )
        ctx.node_executions.append(node_exec_${index + 1})
        
        try:
            node_start = datetime.utcnow()
            ${varName} = await ${functionName}(ctx, ${config})
            node_end = datetime.utcnow()
            
            node_exec_${index + 1}.status = "success"
            node_exec_${index + 1}.completed_at = node_end.isoformat()
            node_exec_${index + 1}.duration_ms = int((node_end - node_start).total_seconds() * 1000)
            node_exec_${index + 1}.output = ${varName} if isinstance(${varName}, dict) else {"result": ${varName}}
            
            logger.info(f"Node '${node.name}' completed successfully")
        except Exception as e:
            node_end = datetime.utcnow()
            node_exec_${index + 1}.status = "error"
            node_exec_${index + 1}.completed_at = node_end.isoformat()
            node_exec_${index + 1}.duration_ms = int((node_end - node_start).total_seconds() * 1000)
            node_exec_${index + 1}.error_message = str(e)
            node_exec_${index + 1}.error_stack = traceback.format_exc()
            
            logger.error(f"Node '${node.name}' failed: {e}")
            raise`;
  }).join('\n\n');
  
  // Last result is the workflow output
  const lastResultVar = `result_${executableNodes.length}`;
  
  return `
@app.post("/workflow/${sanitizeWorkflowName(workflow.name)}")
async def workflow_${sanitizeWorkflowName(workflow.name)}(input_data: dict):
    """
    ${workflow.description || 'Generated workflow endpoint'}
    
    Workflow: ${workflow.name}
    Nodes: ${executableNodes.length}
    """
    # Generate execution ID
    execution_id = str(uuid.uuid4())
    started_at = datetime.utcnow().isoformat()
    
    logger.info(f"Starting workflow: ${workflow.name} (execution: {execution_id})")
    
    # Initialize execution context
    # In production, load secrets from environment variables
    ctx = ExecutionContext(
        secrets={
            "GROQ_API_KEY": os.getenv("GROQ_API_KEY"),
            "ANTHROPIC_API_KEY": os.getenv("ANTHROPIC_API_KEY"),
            "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY"),
        },
        input_data=input_data,
        node_executions=[],
    )
    
    # Prepare execution data for logging
    execution_data = {
        "id": execution_id,
        "workflowId": "${workflow.id}",
        "workflowName": "${workflow.name}",
        "status": "running",
        "startedAt": started_at,
        "trigger": {
            "type": "${workflow.trigger?.type || 'httpEndpoint'}",
            "data": input_data
        },
        "nodeExecutions": []
    }
    
    try:
${nodeExecutions}
        
        # Mark execution as successful
        execution_data["status"] = "success"
        execution_data["completedAt"] = datetime.utcnow().isoformat()
        execution_data["durationMs"] = int(
            (datetime.fromisoformat(execution_data["completedAt"]) - 
             datetime.fromisoformat(started_at)).total_seconds() * 1000
        )
        execution_data["nodeExecutions"] = [
            {
                "nodeId": ne.node_id,
                "nodeName": ne.node_name,
                "nodeType": ne.node_type,
                "status": ne.status,
                "startedAt": ne.started_at,
                "completedAt": ne.completed_at,
                "durationMs": ne.duration_ms,
                "input": ne.input,
                "output": ne.output,
                "error": {
                    "message": ne.error_message,
                    "stack": ne.error_stack
                } if ne.error_message else None
            }
            for ne in ctx.node_executions
        ]
        
        logger.info(f"Workflow completed successfully (execution: {execution_id})")
        
        # Log execution asynchronously (don't block response)
        asyncio.create_task(log_execution_to_catalyst(execution_data))
        
        # Output execution markers for Catalyst local execution capture
        # These markers allow the Electron app to extract execution data from stdout
        import json
        print("__CATALYST_EXECUTION_START__")
        print(json.dumps(execution_data))
        print("__CATALYST_EXECUTION_END__")
        
        return {
            "status": "success",
            "workflow": "${workflow.name}",
            "executionId": execution_id,
            "result": ${lastResultVar},
        }
        
    except Exception as e:
        # Mark execution as failed
        execution_data["status"] = "error"
        execution_data["completedAt"] = datetime.utcnow().isoformat()
        execution_data["error"] = {
            "message": str(e),
            "stack": traceback.format_exc()
        }
        execution_data["nodeExecutions"] = [
            {
                "nodeId": ne.node_id,
                "nodeName": ne.node_name,
                "nodeType": ne.node_type,
                "status": ne.status,
                "startedAt": ne.started_at,
                "completedAt": ne.completed_at,
                "durationMs": ne.duration_ms,
                "input": ne.input,
                "output": ne.output,
                "error": {
                    "message": ne.error_message,
                    "stack": ne.error_stack
                } if ne.error_message else None
            }
            for ne in ctx.node_executions
        ]
        
        logger.error(f"Workflow failed (execution: {execution_id}): {e}")
        
        # Log failed execution asynchronously
        asyncio.create_task(log_execution_to_catalyst(execution_data))
        
        raise HTTPException(status_code=500, detail=str(e))
`;
}

/**
 * Assemble complete Python file
 * 
 * Combines header, imports, library modules, and workflow logic
 * into a complete, runnable Python file.
 * 
 * @param workflow - Workflow definition
 * @param nodes - Array of nodes in workflow
 * @param libraryModules - Generated library code modules
 * @param workflowLogic - Workflow execution endpoint
 * @param warnings - Array to collect warnings
 * @returns Complete Python file as string
 */
function assembleCompleteFile(
  workflow: WorkflowDefinition,
  nodes: NodeDefinition[],
  libraryModules: string[],
  workflowLogic: string,
  warnings: string[]
): string {
  // File header
  const header = `"""
Catalyst Workflow: ${workflow.name}

${workflow.description || 'No description provided'}

@catalyst:generated
DO NOT EDIT - Generated by Catalyst
Changes will be overwritten on next generation

Generated: ${new Date().toISOString()}
Nodes: ${Object.keys(workflow.nodes).length}
Trigger: ${workflow.trigger?.type || 'httpEndpoint'}
"""

import os
import sys
import json
import logging
import asyncio
import uuid
import traceback
from datetime import datetime
from typing import Any, Dict, Optional, List
from fastapi import FastAPI, HTTPException
from dataclasses import dataclass, field
import httpx

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


# ============================================================================
# EXECUTION LOGGING
# ============================================================================

# Catalyst editor URL for execution logging
CATALYST_EDITOR_URL = os.getenv('CATALYST_EDITOR_URL', 'http://localhost:3000')

async def log_execution_to_catalyst(execution_data: Dict[str, Any]):
    """
    Send execution data back to Catalyst editor for logging.
    
    This is non-blocking and will not fail the workflow if logging fails.
    """
    try:
        async with httpx.AsyncClient() as client:
            await client.post(
                f"{CATALYST_EDITOR_URL}/api/executions",
                json=execution_data,
                timeout=2.0
            )
        logger.debug(f"Logged execution {execution_data.get('id')} to Catalyst")
    except Exception as e:
        # Don't fail workflow if logging fails
        logger.warning(f"Failed to log execution to Catalyst: {e}")


# ============================================================================
# EXECUTION CONTEXT
# ============================================================================

@dataclass
class NodeExecution:
    """Tracks execution of a single node."""
    node_id: str
    node_name: str
    node_type: str
    status: str = "pending"  # pending, running, success, error, skipped
    started_at: Optional[str] = None
    completed_at: Optional[str] = None
    duration_ms: Optional[int] = None
    input: Dict[str, Any] = field(default_factory=dict)
    output: Optional[Dict[str, Any]] = None
    error_message: Optional[str] = None
    error_stack: Optional[str] = None


@dataclass
class ExecutionContext:
    """
    Execution context passed to all node functions.
    
    Contains:
    - secrets: API keys and credentials
    - input_data: Input data from workflow trigger
    - state: Workflow state (for future use)
    - node_executions: Track node execution history
    """
    secrets: Dict[str, str]
    input_data: Dict[str, Any]
    state: Dict[str, Any] = None
    node_executions: List[NodeExecution] = field(default_factory=list)
    
    def __post_init__(self):
        if self.state is None:
            self.state = {}


# ============================================================================
# FASTAPI APPLICATION
# ============================================================================

app = FastAPI(
    title="${workflow.name}",
    description="${workflow.description || 'Catalyst Generated Workflow'}",
    version="1.0.0",
)


# Health check endpoint
@app.get("/health")
async def health_check():
    """Health check endpoint for monitoring."""
    return {
        "status": "healthy",
        "workflow": "${workflow.name}",
    }
`;

  // Library modules section
  const librarySection = `
# ============================================================================
# NODE LIBRARY FUNCTIONS
# ============================================================================
# Generated library functions for workflow nodes
# Each node type has execute_* and stream_* functions

${libraryModules.join('\n\n')}
`;

  // Test execution function (must be declared before use in workflowSection)
  const testExecutionFunction = generateTestExecutionFunction(workflow, nodes);
  
  // Workflow logic section
  const workflowSection = `
# ============================================================================
# WORKFLOW ENDPOINTS
# ============================================================================

${workflowLogic}

${testExecutionFunction}
`;

  // Main section with mode detection
  const mainSection = `
# ============================================================================
# MAIN
# ============================================================================

if __name__ == "__main__":
    # Detect execution mode from environment variable
    # - 'test': Execute once with stdin data and exit (for local testing)
    # - 'production': Start FastAPI server (default, for deployment)
    execution_mode = os.getenv('CATALYST_EXECUTION_MODE', 'production')
    
    if execution_mode == 'test':
        # TEST MODE: Execute workflow once and exit
        logger.info("Running in TEST mode (one-shot execution)")
        
        try:
            # Read trigger data from stdin
            import sys
            trigger_data_str = sys.stdin.read()
            trigger_data = json.loads(trigger_data_str)
            
            logger.info(f"Received trigger data: {trigger_data}")
            
            # Execute workflow synchronously
            execution_result = asyncio.run(execute_workflow_test(trigger_data))
            
            # Print execution result with markers for WorkflowExecutor to parse
            print("__CATALYST_EXECUTION_START__")
            print(json.dumps(execution_result, indent=2))
            print("__CATALYST_EXECUTION_END__")
            
            # Exit successfully
            sys.exit(0)
            
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse trigger data from stdin: {e}")
            print("__CATALYST_EXECUTION_START__")
            print(json.dumps({
                "status": "error",
                "error": {
                    "message": f"Invalid JSON in trigger data: {str(e)}",
                    "type": "JSONDecodeError"
                }
            }))
            print("__CATALYST_EXECUTION_END__")
            sys.exit(1)
            
        except Exception as e:
            logger.error(f"Test execution failed: {e}")
            print("__CATALYST_EXECUTION_START__")
            print(json.dumps({
                "status": "error",
                "error": {
                    "message": str(e),
                    "type": type(e).__name__,
                    "stack": traceback.format_exc()
                }
            }))
            print("__CATALYST_EXECUTION_END__")
            sys.exit(1)
    
    else:
        # PRODUCTION MODE: Start FastAPI server
        import uvicorn
        
        logger.info("Running in PRODUCTION mode (FastAPI server)")
        
        # Check for required environment variables
        required_env_vars = ["GROQ_API_KEY"]  # Adjust based on nodes used
        missing_vars = [var for var in required_env_vars if not os.getenv(var)]
        
        if missing_vars:
            logger.error(f"Missing required environment variables: {', '.join(missing_vars)}")
            logger.error("Please set these variables before running the workflow")
            exit(1)
        
        logger.info("Starting Catalyst workflow server...")
        logger.info(f"Workflow: ${workflow.name}")
        logger.info(f"Nodes: ${Object.keys(workflow.nodes).length}")
        
        # Start server
        uvicorn.run(
            app,
            host="0.0.0.0",
            port=8000,
            log_level="info",
        )
`;

  // Combine all sections
  return header + librarySection + workflowSection + mainSection;
}

/**
 * Get execution function name for a node type
 * 
 * Maps node types to their execute_* function names
 * 
 * @param nodeType - Node type
 * @returns Python function name
 */
function getExecutionFunctionName(nodeType: NodeType): string {
  // Map node types to function names
  const functionNames: Record<NodeType, string> = {
    groqCompletion: 'execute_groq_completion',
    anthropicCompletion: 'execute_anthropic_completion',
    openaiCompletion: 'execute_openai_completion',
    embeddingGenerate: 'execute_embedding_generation',
    promptTemplate: 'execute_prompt_template',
    llmRouter: 'execute_llm_router',
    azureOpenaiCompletion: 'execute_azure_openai_completion',
    agenticToolCall: 'execute_agentic_tool_call',
    
    // Triggers
    httpEndpoint: 'execute_http_endpoint',
    scheduledTask: 'execute_scheduled_task',
    webhookReceiver: 'execute_webhook_receiver',
    subworkflowTrigger: 'execute_subworkflow_trigger',
    websocketEndpoint: 'execute_websocket_endpoint',
    queueConsumer: 'execute_queue_consumer',
    // Data
    qdrantSearch: 'execute_qdrant_search',
    qdrantUpsert: 'execute_qdrant_upsert',
    qdrantScroll: 'execute_qdrant_scroll',
    qdrantPayload: 'execute_qdrant_payload',
    postgresQuery: 'execute_postgres_query',
    directusQuery: 'execute_directus_query',
    graphqlQuery: 'execute_graphql_query',
    redisOperation: 'execute_redis_operation',
    // HTTP
    httpRequest: 'execute_http_request',
    gmailOperation: 'execute_gmail_operation',
    webhookSend: 'execute_webhook_send',
    graphqlMutation: 'execute_graphql_mutation',
    // Control Flow
    condition: 'execute_condition',
    switch: 'execute_switch',
    loop: 'execute_loop',
    parallel: 'execute_parallel',
    aggregate: 'execute_aggregate',
    retry: 'execute_retry',
    delay: 'execute_delay',
    earlyReturn: 'execute_early_return',
    // Transform
    editFields: 'execute_edit_fields',
    javascriptFunction: 'execute_javascript_function',
    pythonFunction: 'execute_python_function',
    jsonTransform: 'execute_json_transform',
    mapArray: 'execute_map_array',
    filterArray: 'execute_filter_array',
    reduceArray: 'execute_reduce_array',
    splitArray: 'execute_split_array',
    // Streaming
    streamStart: 'execute_stream_start',
    streamChunk: 'execute_stream_chunk',
    streamEnd: 'execute_stream_end',
    streamMerge: 'execute_stream_merge',
    streamBuffer: 'execute_stream_buffer',
    // Utilities
    cryptoGenerate: 'execute_crypto_generate',
    executionData: 'execute_execution_data',
    globalVariable: 'execute_global_variable',
    errorHandler: 'execute_error_handler',
    log: 'execute_log',
    metrics: 'execute_metrics',
    rateLimit: 'execute_rate_limit',
    validate: 'execute_validate',
  };
  
  return functionNames[nodeType] || 'execute_unknown_node';
}

/**
 * Generate minimal test execution function for trigger-only workflows
 * 
 * Used when a workflow contains only trigger nodes (no executable nodes).
 * Returns input data immediately without any node execution.
 * 
 * @param workflow - Workflow definition
 * @returns Python code for minimal test execution function
 */
function generateMinimalTestExecutionFunction(workflow: WorkflowDefinition): string {
  return `
# ============================================================================
# TEST EXECUTION FUNCTION
# ============================================================================

async def execute_workflow_test(trigger_data: dict) -> dict:
    """
    Execute workflow once with provided trigger data (test mode).
    
    Note: This workflow contains only trigger nodes (no executable logic).
    Returns input data immediately.
    """
    # Generate execution ID
    execution_id = str(uuid.uuid4())
    started_at = datetime.utcnow()
    
    logger.info(f"[TEST MODE] Starting workflow: ${workflow.name} (execution: {execution_id})")
    logger.info("[TEST MODE] Workflow contains only trigger nodes, no execution needed")
    
    # Minimal execution result
    execution_result = {
        "id": execution_id,
        "workflowId": "${workflow.id}",
        "workflowName": "${workflow.name}",
        "status": "success",
        "startedAt": started_at.isoformat(),
        "completedAt": datetime.utcnow().isoformat(),
        "durationMs": 0,
        "trigger": {
            "type": "${workflow.trigger?.type || 'httpEndpoint'}",
            "data": trigger_data
        },
        "nodeExecutions": [],
        "output": trigger_data,
    }
    
    logger.info(f"[TEST MODE] Workflow completed (execution: {execution_id})")
    
    return execution_result
`;
}

/**
 * Generate test execution function
 * 
 * Creates an async function that executes the workflow once with provided
 * trigger data, without starting a FastAPI server. Used for local testing.
 * 
 * DESIGN:
 * - Takes trigger data as parameter (from stdin in test mode)
 * - Executes nodes in topological order (excluding triggers)
 * - Captures execution details (timing, inputs, outputs, errors)
 * - Returns structured execution JSON
 * - No FastAPI dependencies
 * 
 * @param workflow - Workflow definition
 * @param nodes - Array of nodes to execute
 * @returns Python code for test execution function
 */
function generateTestExecutionFunction(
  workflow: WorkflowDefinition,
  nodes: NodeDefinition[]
): string {
  // Filter out trigger nodes (same logic as FastAPI endpoint)
  const executableNodes = nodes.filter(node => !isTriggerNode(node.type));
  
  // If no executable nodes, return minimal test function
  if (executableNodes.length === 0) {
    return generateMinimalTestExecutionFunction(workflow);
  }
  
  // Generate node execution calls (same logic as FastAPI endpoint)
  const nodeExecutions = executableNodes.map((node, index) => {
    const varName = `result_${index + 1}`;
    const functionName = getExecutionFunctionName(node.type);
    const config = JSON.stringify(node.config, null, 4);
    
    return `        # Node: ${node.name} (${node.type})
        node_exec_${index + 1} = NodeExecution(
            node_id="${node.id}",
            node_name="${node.name}",
            node_type="${node.type}",
            status="running",
            started_at=datetime.utcnow().isoformat(),
            input={}  # TODO: Extract actual input from config
        )
        ctx.node_executions.append(node_exec_${index + 1})
        
        try:
            node_start = datetime.utcnow()
            ${varName} = await ${functionName}(ctx, ${config})
            node_end = datetime.utcnow()
            
            node_exec_${index + 1}.status = "success"
            node_exec_${index + 1}.completed_at = node_end.isoformat()
            node_exec_${index + 1}.duration_ms = int((node_end - node_start).total_seconds() * 1000)
            node_exec_${index + 1}.output = ${varName} if isinstance(${varName}, dict) else {"result": ${varName}}
            
            logger.info(f"Node '${node.name}' completed successfully")
        except Exception as e:
            node_end = datetime.utcnow()
            node_exec_${index + 1}.status = "error"
            node_exec_${index + 1}.completed_at = node_end.isoformat()
            node_exec_${index + 1}.duration_ms = int((node_end - node_start).total_seconds() * 1000)
            node_exec_${index + 1}.error_message = str(e)
            node_exec_${index + 1}.error_stack = traceback.format_exc()
            
            logger.error(f"Node '${node.name}' failed: {e}")
            raise`;
  }).join('\n\n');
  
  // Last result is the workflow output
  const lastResultVar = `result_${executableNodes.length}`;
  
  return `
# ============================================================================
# TEST EXECUTION FUNCTION
# ============================================================================

async def execute_workflow_test(trigger_data: dict) -> dict:
    """
    Execute workflow once with provided trigger data (test mode).
    
    This function is used for local testing and bypasses FastAPI.
    It executes the workflow synchronously and returns the execution result.
    
    Args:
        trigger_data: Simulated HTTP request or other trigger data
        
    Returns:
        Execution result with node executions, timing, and output
        
    Used by:
        - Catalyst local execution runner (WorkflowExecutor)
        - Manual testing with CATALYST_EXECUTION_MODE=test
        
    Example trigger_data:
        {
            "method": "POST",
            "path": "/api/search",
            "headers": {"Content-Type": "application/json"},
            "body": {"query": "test"},
            "query": {}
        }
    """
    # Generate execution ID
    execution_id = str(uuid.uuid4())
    started_at = datetime.utcnow()
    started_at_iso = started_at.isoformat()
    
    logger.info(f"[TEST MODE] Starting workflow: ${workflow.name} (execution: {execution_id})")
    
    # Initialize execution context
    ctx = ExecutionContext(
        secrets={
            "GROQ_API_KEY": os.getenv("GROQ_API_KEY"),
            "ANTHROPIC_API_KEY": os.getenv("ANTHROPIC_API_KEY"),
            "OPENAI_API_KEY": os.getenv("OPENAI_API_KEY"),
        },
        input_data=trigger_data,
        node_executions=[],
    )
    
    try:
${nodeExecutions}
        
        # Calculate execution duration
        completed_at = datetime.utcnow()
        duration_ms = int((completed_at - started_at).total_seconds() * 1000)
        
        # Build execution result
        execution_result = {
            "id": execution_id,
            "workflowId": "${workflow.id}",
            "workflowName": "${workflow.name}",
            "status": "success",
            "startedAt": started_at_iso,
            "completedAt": completed_at.isoformat(),
            "durationMs": duration_ms,
            "trigger": {
                "type": "${workflow.trigger?.type || 'httpEndpoint'}",
                "data": trigger_data
            },
            "nodeExecutions": [
                {
                    "nodeId": ne.node_id,
                    "nodeName": ne.node_name,
                    "nodeType": ne.node_type,
                    "status": ne.status,
                    "startedAt": ne.started_at,
                    "completedAt": ne.completed_at,
                    "durationMs": ne.duration_ms,
                    "input": ne.input,
                    "output": ne.output,
                    "error": {
                        "message": ne.error_message,
                        "stack": ne.error_stack
                    } if ne.error_message else None
                }
                for ne in ctx.node_executions
            ],
            "output": ${lastResultVar} if isinstance(${lastResultVar}, dict) else {"result": ${lastResultVar}},
        }
        
        logger.info(f"[TEST MODE] Workflow completed successfully (execution: {execution_id})")
        
        return execution_result
        
    except Exception as e:
        # Calculate execution duration
        completed_at = datetime.utcnow()
        duration_ms = int((completed_at - started_at).total_seconds() * 1000)
        
        # Build error execution result
        execution_result = {
            "id": execution_id,
            "workflowId": "${workflow.id}",
            "workflowName": "${workflow.name}",
            "status": "error",
            "startedAt": started_at_iso,
            "completedAt": completed_at.isoformat(),
            "durationMs": duration_ms,
            "trigger": {
                "type": "${workflow.trigger?.type || 'httpEndpoint'}",
                "data": trigger_data
            },
            "error": {
                "message": str(e),
                "type": type(e).__name__,
                "stack": traceback.format_exc()
            },
            "nodeExecutions": [
                {
                    "nodeId": ne.node_id,
                    "nodeName": ne.node_name,
                    "nodeType": ne.node_type,
                    "status": ne.status,
                    "startedAt": ne.started_at,
                    "completedAt": ne.completed_at,
                    "durationMs": ne.duration_ms,
                    "input": ne.input,
                    "output": ne.output,
                    "error": {
                        "message": ne.error_message,
                        "stack": ne.error_stack
                    } if ne.error_message else None
                }
                for ne in ctx.node_executions
            ],
        }
        
        logger.error(f"[TEST MODE] Workflow failed (execution: {execution_id}): {e}")
        
        return execution_result
`;
}

/**
 * Sanitize workflow name for use in Python identifiers
 * 
 * Converts workflow name to valid Python identifier:
 * - Lowercase
 * - Replace spaces/special chars with underscores
 * - Remove consecutive underscores
 * 
 * @param name - Workflow name
 * @returns Sanitized name
 * 
 * @example
 * sanitizeWorkflowName("My Workflow!") // "my_workflow"
 * sanitizeWorkflowName("User-API-v2") // "user_api_v2"
 */
function sanitizeWorkflowName(name: string): string {
  return name
    .toLowerCase()
    .replace(/[^a-z0-9]+/g, '_')  // Replace non-alphanumeric with underscore
    .replace(/^_+|_+$/g, '')       // Remove leading/trailing underscores
    .replace(/_+/g, '_')           // Remove consecutive underscores
    || 'workflow';                 // Fallback if name is empty
}
